{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:07.772565Z",
     "start_time": "2024-05-31T06:50:07.764085Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "import json\n",
    "import torch.nn as nn\n",
    "from c2nl.eval.bleu import corpus_bleu, compute_bleu\n",
    "from c2nl.eval.rouge import Rouge\n",
    "from c2nl.eval.meteor import Meteor\n",
    "from c2nl.eval.distinct_n.distinct_ngrams import distinct_n_corpus_level\n",
    "from c2nl.eval.self_bleu import self_bleu_score\n",
    "from itertools import islice, zip_longest, chain\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer, get_linear_schedule_with_warmup, T5ForConditionalGeneration\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "from evaluate import load\n",
    "from transformers.models.t5.modeling_t5 import T5Stack, T5LayerCrossAttention, T5LayerNorm, T5LayerFF\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType, PeftModel, PeftConfig#, PeftModelForSeq2SeqLM\n",
    "from peft.utils import (\n",
    "    SAFETENSORS_WEIGHTS_NAME,\n",
    "    TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING,\n",
    "    WEIGHTS_NAME,\n",
    "    PeftType,\n",
    "    TaskType,\n",
    "    _get_batch_size,\n",
    "    _prepare_prompt_learning_config,\n",
    "    _set_adapter,\n",
    "    _set_trainable,\n",
    "    get_peft_model_state_dict,\n",
    "    id_tensor_storage,\n",
    "    infer_device,\n",
    "    load_peft_weights,\n",
    "    set_peft_model_state_dict,\n",
    "    shift_tokens_right,\n",
    ")\n",
    "import warnings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from typing import Any, Optional, Union\n",
    "from copy import deepcopy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:10.254841Z",
     "start_time": "2024-05-31T06:50:07.814942Z"
    }
   },
   "id": "f4508f0b329b2a45",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value)  # Numpy module\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    torch.cuda.manual_seed(seed_value)  # PyTorch CUDA\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch CUDA (for multi-GPU setups)\n",
    "    torch.backends.cudnn.deterministic = True  # For CUDA backend\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # For Python hash seeding\n",
    "\n",
    "# Example usage\n",
    "set_seed(42)  # Replace 42 with your desired seed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:10.260543Z",
     "start_time": "2024-05-31T06:50:10.256026Z"
    }
   },
   "id": "dd2b93a6082948c2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "lang = \"python\"\n",
    "# num_prefix = 2\n",
    "num_virtual_tokens = 2\n",
    "if len(lang.split('_')) > 1:\n",
    "    langs = lang.split('_')\n",
    "else:\n",
    "    langs = [lang]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "checkpoint_dir = \"./codet5p_checkpoints\"\n",
    "checkpoint_name = f\"codet5p_VPT_lang_{lang}_num_vtokens_{num_virtual_tokens}\"\n",
    "\n",
    "# Your finetuned model\n",
    "base_model = f\"codet5p_ft_lang_{lang}_backbone\"\n",
    "\n",
    "if lang == 'java':\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m'\n",
    "else:\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m-bimodal'\n",
    "if 'bimodal' in base_model or 'python' in base_model:\n",
    "    print(\"using auto model\")\n",
    "    backbone_model = AutoModel.from_pretrained(base_model, trust_remote_code=True).to(device)\n",
    "else:\n",
    "    print(\"using t5 conditional generation model\")\n",
    "    backbone_model = T5ForConditionalGeneration.from_pretrained(base_model).to(device)\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "train_batch_size = 16\n",
    "test_batch_size = 64\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_tokenizer)\n",
    "print(checkpoint_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:11.810589Z",
     "start_time": "2024-05-31T06:50:10.261443Z"
    }
   },
   "id": "4fda6d29cc42617b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using auto model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codet5p_full_cvae_ft_prefix_tuning_self_attend_cond_code_prior_lang_python_num_vtokens_2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "print(torch.cuda.get_device_name(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:11.815157Z",
     "start_time": "2024-05-31T06:50:11.812143Z"
    }
   },
   "id": "184f535ac84fb2d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# load data, change data directory here, make sure its format follows the one given in data directory\n",
    "train_codes = []\n",
    "train_docs = []\n",
    "for l in langs:\n",
    "    train_codes.extend(open(\"./data/{}/train/code.original\".format(l), 'r').readlines())\n",
    "    train_docs.extend(open(\"./data/{}/train/javadoc.original\".format(l), 'r').readlines())\n",
    "train_codes = [code.rstrip() for code in train_codes]\n",
    "train_docs = [doc.rstrip() for doc in train_docs]\n",
    "train_docs_codes = [code + tokenizer.eos_token + doc for code, doc in zip(train_codes, train_docs)]\n",
    "train_inputs = {\n",
    "    \"code_text\": train_codes,\n",
    "    \"doc_text\": train_docs,\n",
    "    \"doc_code_text\": train_docs_codes\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:11.912909Z",
     "start_time": "2024-05-31T06:50:11.816303Z"
    }
   },
   "id": "a90369e5c2cd7e9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = Dataset.from_dict(train_inputs)\n",
    "train_data.set_format(type='torch', columns=['code_text', 'doc_text', 'doc_code_text'])\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:11.991436Z",
     "start_time": "2024-05-31T06:50:11.913848Z"
    }
   },
   "id": "d16ecead06b295e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['code_text', 'doc_text', 'doc_code_text'],\n",
       "    num_rows: 55538\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "train_loader = DataLoader(train_data, batch_size=train_batch_size, shuffle=True)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:11.994789Z",
     "start_time": "2024-05-31T06:50:11.992357Z"
    }
   },
   "id": "bbbc25db4e205ee4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "train_ex_batch = next(iter(train_loader))\n",
    "print(train_ex_batch.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.001979Z",
     "start_time": "2024-05-31T06:50:11.995973Z"
    }
   },
   "id": "9f6467d28e41e9e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['code_text', 'doc_text', 'doc_code_text'])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "ex_codes_input = tokenizer(train_ex_batch['code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "ex_docs_input = tokenizer(train_ex_batch['doc_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "ex_docs_codes_input = tokenizer(train_ex_batch['doc_code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length+max_target_length)\n",
    "ex_input_ids = ex_codes_input[\"input_ids\"]\n",
    "ex_attention_mask = ex_codes_input[\"attention_mask\"]\n",
    "ex_labels = ex_docs_input[\"input_ids\"].clone()\n",
    "ex_labels[ex_labels == tokenizer.pad_token_id] = -100\n",
    "ex_labels_ids = ex_docs_input[\"input_ids\"]\n",
    "ex_labels_attention_mask = ex_docs_input[\"attention_mask\"]\n",
    "ex_docs_codes_input_ids = ex_docs_codes_input[\"input_ids\"]\n",
    "ex_docs_codes_attention_mask = ex_docs_codes_input[\"attention_mask\"]\n",
    "(ex_input_ids.shape, ex_attention_mask.shape, ex_labels.shape, ex_labels_ids.shape, ex_labels_attention_mask.shape, ex_docs_codes_input_ids.shape, ex_docs_codes_attention_mask.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.022641Z",
     "start_time": "2024-05-31T06:50:12.002767Z"
    }
   },
   "id": "d962a7e220d4fdab",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 223]),\n",
       " torch.Size([15, 223]),\n",
       " torch.Size([15, 39]),\n",
       " torch.Size([15, 39]),\n",
       " torch.Size([15, 39]),\n",
       " torch.Size([15, 229]),\n",
       " torch.Size([15, 229]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = 5e-5\n",
    "warmup_steps = 500\n",
    "num_epochs = 200\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.027089Z",
     "start_time": "2024-05-31T06:50:12.024976Z"
    }
   },
   "id": "93def6bddc5185f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_bleu(model, device, src_dir, tgt_dir, tokenizer, std_scale=1.0):\n",
    "    model.eval()\n",
    "    model.std_scale = std_scale\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    all_summaries = []\n",
    "    eval_inputs = {\n",
    "        \"code_text\": source_codes,\n",
    "        \"doc_text\": targets\n",
    "    }\n",
    "    eval_data = Dataset.from_dict(eval_inputs)\n",
    "    eval_data.set_format(type='torch', columns=['code_text', 'doc_text'])\n",
    "    eval_loader = DataLoader(eval_data, batch_size=test_batch_size)\n",
    "    for batch in tqdm(eval_loader):\n",
    "        tokenized_codes = tokenizer(batch['code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        tokenized_docs = tokenizer(batch['doc_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "        input_ids = tokenized_codes[\"input_ids\"].to(device)\n",
    "        # print(input_ids.shape)\n",
    "        attention_mask = tokenized_codes[\"attention_mask\"].to(device)\n",
    "        labels = tokenized_docs[\"input_ids\"].clone().to(device)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        labels_ids = tokenized_docs[\"input_ids\"].to(device)\n",
    "        labels_attention_mask = tokenized_docs[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids=input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=80)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[summary.rstrip().lower()[:-1]+' .'] for summary in all_summaries]))\n",
    "    references = dict(enumerate([[target.rstrip().lower()] for target in targets]))\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    return bleu\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.033927Z",
     "start_time": "2024-05-31T06:50:12.027973Z"
    }
   },
   "id": "20c792feb8e120d9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.036544Z",
     "start_time": "2024-05-31T06:50:12.034785Z"
    }
   },
   "id": "eec0230d883b879e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_bleu_with_answer(model, device, src_dir, tgt_dir, tokenizer):\n",
    "    model.eval()\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    all_summaries = []\n",
    "    eval_inputs = {\n",
    "        \"code_text\": source_codes,\n",
    "        \"doc_text\": targets\n",
    "    }\n",
    "    eval_data = Dataset.from_dict(eval_inputs)\n",
    "    eval_data.set_format(type='torch', columns=['code_text', 'doc_text'])\n",
    "    eval_loader = DataLoader(eval_data, batch_size=test_batch_size)\n",
    "    for batch in tqdm(eval_loader):\n",
    "        tokenized_codes = tokenizer(batch['code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        tokenized_docs = tokenizer(batch['doc_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "        input_ids = tokenized_codes[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized_codes[\"attention_mask\"].to(device)\n",
    "        labels = tokenized_docs[\"input_ids\"].clone().to(device)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        labels_ids = tokenized_docs[\"input_ids\"].to(device)\n",
    "        labels_attention_mask = tokenized_docs[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids=input_ids, \n",
    "                                       labels=labels,\n",
    "                                       max_length=50)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[summary.rstrip().lower()[:-1]+' .'] for summary in all_summaries]))\n",
    "    references = dict(enumerate([[target.rstrip().lower()] for target in targets]))\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    return bleu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.043099Z",
     "start_time": "2024-05-31T06:50:12.037411Z"
    }
   },
   "id": "48d020acf13bf500",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=num_virtual_tokens)",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.046014Z",
     "start_time": "2024-05-31T06:50:12.043977Z"
    }
   },
   "id": "a526ac8654ec93c7",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "class CVAEPrefixEncoder(torch.nn.Module):\n",
    "    r\"\"\"\n",
    "    The `torch.nn` model to encode the prefix.\n",
    "\n",
    "    Args:\n",
    "        config ([`PrefixTuningConfig`]): The configuration of the prefix encoder.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```py\n",
    "    >>> from peft import PrefixEncoder, PrefixTuningConfig\n",
    "\n",
    "    >>> config = PrefixTuningConfig(\n",
    "    ...     peft_type=\"PREFIX_TUNING\",\n",
    "    ...     task_type=\"SEQ_2_SEQ_LM\",\n",
    "    ...     num_virtual_tokens=20,\n",
    "    ...     token_dim=768,\n",
    "    ...     num_transformer_submodules=1,\n",
    "    ...     num_attention_heads=12,\n",
    "    ...     num_layers=12,\n",
    "    ...     encoder_hidden_size=768,\n",
    "    ... )\n",
    "    >>> prefix_encoder = PrefixEncoder(config)\n",
    "    ```\n",
    "\n",
    "    **Attributes**:\n",
    "        - **embedding** (`torch.nn.Embedding`) -- The embedding layer of the prefix encoder.\n",
    "        - **transform** (`torch.nn.Sequential`) -- The two-layer MLP to transform the prefix embeddings if\n",
    "          `prefix_projection` is `True`.\n",
    "        - **prefix_projection** (`bool`) -- Whether to project the prefix embeddings.\n",
    "\n",
    "    Input shape: (`batch_size`, `num_virtual_tokens`)\n",
    "\n",
    "    Output shape: (`batch_size`, `num_virtual_tokens`, `2*layers*hidden`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        super().__init__()\n",
    "        # self.prefix_projection = config.prefix_projection\n",
    "        text_encoder = model.get_encoder()\n",
    "        self.token_dim = config.token_dim\n",
    "        self.num_layers = config.num_layers\n",
    "        self.d_model = text_encoder.config.d_model\n",
    "        # encoder_hidden_size = config.encoder_hidden_size\n",
    "        self.num_virtual_tokens = config.num_virtual_tokens\n",
    "        # if self.prefix_projection and not config.inference_mode:\n",
    "        #     # Use a two-layer MLP to encode the prefix\n",
    "        #     self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)\n",
    "        #     self.transform = torch.nn.Sequential(\n",
    "        #         torch.nn.Linear(token_dim, encoder_hidden_size),\n",
    "        #         torch.nn.Tanh(),\n",
    "        #         torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),\n",
    "        #     )\n",
    "        # else:\n",
    "        #     self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)\n",
    "        self.embedding = nn.Embedding(self.num_virtual_tokens, self.d_model)\n",
    "        self.code_embedding = nn.Embedding(self.num_virtual_tokens, self.d_model)\n",
    "        self.encoder_embedding = text_encoder.get_input_embeddings()\n",
    "        \n",
    "        self.encoder = deepcopy(text_encoder)\n",
    "        # freeze the encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # summ_encode_layer_config = copy.deepcopy(text_encoder.config)\n",
    "        # summ_encode_layer_config.num_layers = 2\n",
    "        # self.encoder = T5Stack(summ_encode_layer_config, self.encoder_embedding)\n",
    "        \n",
    "        # context_attn_config = copy.deepcopy(model.get_decoder().config)\n",
    "        # context_attn_config.num_layers = 1\n",
    "        # \n",
    "        # self.context_attn = T5LayerCrossAttention(context_attn_config)\n",
    "        # self.dropout = nn.Dropout(model.config.dropout_rate)\n",
    "        # self.layer_norm = T5LayerNorm(model.config.d_model)\n",
    "        \n",
    "        self.mean = T5LayerFF(text_encoder.config)\n",
    "        self.log_var = T5LayerFF(text_encoder.config)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # self.transform_mean = nn.Sequential(\n",
    "        #     nn.Linear(self.d_model, self.token_dim),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(self.token_dim, self.num_layers * 2 * self.d_model),\n",
    "        # )\n",
    "        # \n",
    "        # self.transform_log_var = nn.Sequential(\n",
    "        #     nn.Linear(self.d_model, self.token_dim),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(self.token_dim, self.num_layers * 2 * self.d_model),\n",
    "        # )\n",
    "        \n",
    "        self.transform_z = nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.token_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.token_dim, self.num_layers * 2 * self.d_model),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,  \n",
    "                prefix, \n",
    "                input_ids=None, \n",
    "                attention_mask=None, \n",
    "                labels_id=None, \n",
    "                labels_attention_mask=None,\n",
    "                doc_code_input_ids=None,\n",
    "                doc_code_attention_mask=None,\n",
    "                std_scale=1.0,\n",
    "                num_beams=None):\n",
    "        if self.training:\n",
    "            prefix_embeddings = self.embedding(prefix)\n",
    "            code_prefix_embeddings = self.code_embedding(prefix)\n",
    "            \n",
    "            # labels_embeddings = self.encoder_embedding(labels_id)\n",
    "            input_embeddings = self.encoder_embedding(input_ids)\n",
    "            doc_code_embeddings = self.encoder_embedding(doc_code_input_ids)\n",
    "            doc_code_attention_mask = torch.cat((torch.ones(prefix.shape[0], self.num_virtual_tokens).to(prefix.device), \n",
    "                                                doc_code_attention_mask), dim=1)\n",
    "            \n",
    "            concat_embeddings = torch.cat((prefix_embeddings, doc_code_embeddings), dim=1)\n",
    "            code_concat_embeddings = torch.cat((code_prefix_embeddings, input_embeddings), dim=1)\n",
    "            # concat_mask = torch.cat((labels_attention_mask, attention_mask), dim=1)\n",
    "            \n",
    "            encoder_outputs = self.encoder(input_ids=None,\n",
    "                                           attention_mask=doc_code_attention_mask,\n",
    "                                           inputs_embeds=concat_embeddings)\n",
    "            \n",
    "            # attention_mask_with_prefix = torch.cat((torch.ones(prefix.shape[0], self.num_virtual_tokens).to(prefix.device), \n",
    "            #                                         attention_mask), dim=1)\n",
    "            # code_encoder_outputs = self.encoder(input_ids=input_ids,\n",
    "            #                                     attention_mask=attention_mask)\n",
    "            \n",
    "            # prior_mu = code_encoder_outputs.last_hidden_state[:, :self.num_virtual_tokens]\n",
    "\n",
    "            attention_mask_with_prefix = torch.cat((torch.ones(prefix.shape[0], self.num_virtual_tokens).to(prefix.device),\n",
    "                                                    attention_mask), dim=1)\n",
    "            code_encoder_outputs = self.encoder(input_ids=None,\n",
    "                                                attention_mask=attention_mask_with_prefix,\n",
    "                                                inputs_embeds=code_concat_embeddings)\n",
    "\n",
    "            prior_mu = code_encoder_outputs.last_hidden_state[:, :self.num_virtual_tokens]\n",
    "            \n",
    "            # code_encoding = code_encoder_outputs.last_hidden_state\n",
    "            # past_key_values = self.transform(encoder_outputs.last_hidden_state[:, :self.num_virtual_tokens])\n",
    "            prefix_encoding = encoder_outputs.last_hidden_state[:, :self.num_virtual_tokens]\n",
    "            \n",
    "            # mid = self.context_attn(prefix_encoding,\n",
    "            #                         key_value_states=code_encoding,\n",
    "            #                         attention_mask=self.encoder.get_extended_attention_mask(attention_mask, input_ids.size()))[0]\n",
    "            \n",
    "            # print(mid.shape)\n",
    "            # prefix_encoding = self.layer_norm(self.dropout(mid) + prefix_encoding)\n",
    "                                    \n",
    "            mean = self.mean(prefix_encoding)\n",
    "            log_var = self.log_var(prefix_encoding)\n",
    "            # projected_mean = self.transform_mean(mean)\n",
    "            # projected_log_var = self.transform_log_var(log_var)\n",
    "            std = torch.exp(0.5 * log_var)\n",
    "            eps = torch.randn_like(std)\n",
    "            z = eps.mul(std).add_(mean)\n",
    "            past_key_values = self.transform_z(z)\n",
    "        else:\n",
    "            # past_key_values_shape = (prefix.shape[0], self.num_virtual_tokens, self.num_layers * 2, self.d_model)\n",
    "            # projected_mean = torch.zeros(past_key_values_shape).to(prefix.device)\n",
    "            # projected_log_var = torch.zeros(past_key_values_shape).to(prefix.device)\n",
    "            # past_key_values = torch.empty_like(projected_mean).normal_(mean=0, \n",
    "            #                                                            std=std_scale).to(prefix.device)\n",
    "            \n",
    "            code_prefix_embeddings = self.code_embedding(prefix)\n",
    "            if num_beams:\n",
    "                input_ids = input_ids.repeat_interleave(num_beams, dim=0)\n",
    "            input_embeddings = self.encoder_embedding(input_ids)\n",
    "\n",
    "            code_concat_embeddings = torch.cat((code_prefix_embeddings, input_embeddings), dim=1)\n",
    "\n",
    "            attention_mask_with_prefix = torch.cat((torch.ones(prefix.shape[0], self.num_virtual_tokens).to(prefix.device), \n",
    "                                                    attention_mask), dim=1)\n",
    "            code_encoder_outputs = self.encoder(input_ids=None,\n",
    "                                                attention_mask=attention_mask_with_prefix,\n",
    "                                                inputs_embeds=code_concat_embeddings)\n",
    "            \n",
    "            prior_mu = code_encoder_outputs.last_hidden_state[:, :self.num_virtual_tokens]\n",
    "            \n",
    "            # z_shape = (prefix.shape[0], self.num_virtual_tokens, self.d_model)\n",
    "            # \n",
    "            # z = torch.empty(z_shape).normal_(mean=0, std=std_scale).to(prefix.device) # + prior_mu\n",
    "            # past_key_values = self.transform_z(z)\n",
    "            # mean = torch.zeros(z_shape).to(prefix.device)\n",
    "            # log_var = torch.zeros(z_shape).to(prefix.device)\n",
    "            z = torch.empty_like(prior_mu).normal_(mean=0, std=std_scale).to(prefix.device) + prior_mu\n",
    "            past_key_values = self.transform_z(z)\n",
    "            mean = torch.zeros_like(prior_mu).to(prefix.device)\n",
    "            log_var = torch.zeros_like(prior_mu).to(prefix.device)\n",
    "        # return past_key_values, projected_mean.reshape(prefix.shape[0], 1, -1), projected_log_var.reshape(prefix.shape[0], 1, -1)\n",
    "        return past_key_values, mean, log_var, prior_mu"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.060277Z",
     "start_time": "2024-05-31T06:50:12.046973Z"
    }
   },
   "id": "2023ac78e318475d",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "class CVAEPeftModelForSeq2SeqLM(PeftModel):\n",
    "    \"\"\"\n",
    "    Peft model for sequence-to-sequence language modeling.\n",
    "\n",
    "    Args:\n",
    "        model ([`~transformers.PreTrainedModel`]): Base transformer model.\n",
    "        peft_config ([`PeftConfig`]): Peft config.\n",
    "\n",
    "\n",
    "    Example:\n",
    "\n",
    "        ```py\n",
    "        >>> from transformers import AutoModelForSeq2SeqLM\n",
    "        >>> from peft import PeftModelForSeq2SeqLM, get_peft_config\n",
    "\n",
    "        >>> config = {\n",
    "        ...     \"peft_type\": \"LORA\",\n",
    "        ...     \"task_type\": \"SEQ_2_SEQ_LM\",\n",
    "        ...     \"inference_mode\": False,\n",
    "        ...     \"r\": 8,\n",
    "        ...     \"target_modules\": [\"q\", \"v\"],\n",
    "        ...     \"lora_alpha\": 32,\n",
    "        ...     \"lora_dropout\": 0.1,\n",
    "        ...     \"fan_in_fan_out\": False,\n",
    "        ...     \"enable_lora\": None,\n",
    "        ...     \"bias\": \"none\",\n",
    "        ... }\n",
    "\n",
    "        >>> peft_config = get_peft_config(config)\n",
    "        >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
    "        >>> peft_model = PeftModelForSeq2SeqLM(model, peft_config)\n",
    "        >>> peft_model.print_trainable_parameters()\n",
    "        trainable params: 884736 || all params: 223843584 || trainable%: 0.3952474242013566\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: str = \"default\") -> None:\n",
    "        super().__init__(model, peft_config, adapter_name)\n",
    "        self.curr_input_ids = None\n",
    "        self.base_model_prepare_inputs_for_generation = self.base_model.prepare_inputs_for_generation\n",
    "        self.base_model_prepare_encoder_decoder_kwargs_for_generation = (\n",
    "            self.base_model._prepare_encoder_decoder_kwargs_for_generation\n",
    "        )\n",
    "        self.prompt_encoder[self.active_adapter] = CVAEPrefixEncoder(peft_config, self.base_model)\n",
    "        self.std_scale = 1.0\n",
    "        self.beam_size = None\n",
    "\n",
    "    @staticmethod\n",
    "    def kl_loss(mean1, logvar1, mean2, logvar2):\n",
    "        exponential = logvar1 - logvar2 - torch.pow(mean1 - mean2, 2) / logvar2.exp() - torch.exp(logvar1 - logvar2) + 1\n",
    "        return -0.5 * torch.sum(exponential, tuple(range(1, len(exponential.shape))))\n",
    "\n",
    "    def get_prompt(self, \n",
    "                   batch_size: int, \n",
    "                   task_ids: Optional[torch.Tensor] = None, \n",
    "                   input_ids=None, \n",
    "                   attention_mask=None, \n",
    "                   labels_id=None, \n",
    "                   labels_attention_mask=None, \n",
    "                   doc_code_input_ids=None,\n",
    "                   doc_code_attention_mask=None,\n",
    "                   std_scale=1.0, \n",
    "                   num_beams=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.\n",
    "        \"\"\"\n",
    "        peft_config = self.active_peft_config\n",
    "        prompt_encoder = self.prompt_encoder[self.active_adapter]\n",
    "        prompt_tokens = (\n",
    "            self.prompt_tokens[self.active_adapter]\n",
    "            .unsqueeze(0)\n",
    "            .expand(batch_size, -1)\n",
    "            .to(prompt_encoder.embedding.weight.device)\n",
    "        )\n",
    "\n",
    "        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            prompt_tokens = prompt_tokens[:, : peft_config.num_virtual_tokens]\n",
    "\n",
    "            if peft_config.inference_mode:\n",
    "                past_key_values = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "            else:\n",
    "                past_key_values, mu, logvar, prior_mu = prompt_encoder(prompt_tokens,\n",
    "                                                                       input_ids=input_ids,\n",
    "                                                                       attention_mask=attention_mask,\n",
    "                                                                       labels_id=labels_id,\n",
    "                                                                       labels_attention_mask=labels_attention_mask,\n",
    "                                                                       doc_code_input_ids=doc_code_input_ids,\n",
    "                                                                       doc_code_attention_mask=doc_code_attention_mask,\n",
    "                                                                       std_scale=std_scale,\n",
    "                                                                       num_beams=num_beams)\n",
    "            if self.base_model_torch_dtype is not None:\n",
    "                past_key_values = past_key_values.to(self.base_model_torch_dtype)\n",
    "            past_key_values = past_key_values.view(\n",
    "                batch_size,\n",
    "                peft_config.num_virtual_tokens,\n",
    "                peft_config.num_layers * 2,\n",
    "                peft_config.num_attention_heads,\n",
    "                peft_config.token_dim // peft_config.num_attention_heads,\n",
    "            )\n",
    "            if peft_config.num_transformer_submodules == 2:\n",
    "                past_key_values = torch.cat([past_key_values, past_key_values], dim=2)\n",
    "            past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(\n",
    "                peft_config.num_transformer_submodules * 2\n",
    "            )\n",
    "            if TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING.get(self.config.model_type, None) is not None:\n",
    "                post_process_fn = TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING[self.config.model_type]\n",
    "                past_key_values = post_process_fn(past_key_values)\n",
    "            return past_key_values, mu, logvar, prior_mu\n",
    "        else:\n",
    "            if peft_config.peft_type == PeftType.MULTITASK_PROMPT_TUNING:\n",
    "                prompts = prompt_encoder(prompt_tokens, task_ids)\n",
    "            else:\n",
    "                if peft_config.inference_mode:\n",
    "                    prompts = prompt_encoder.embedding.weight.repeat(batch_size, 1, 1)\n",
    "                else:\n",
    "                    prompts = prompt_encoder(prompt_tokens)\n",
    "            return prompts\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids=None,\n",
    "            attention_mask=None,\n",
    "            inputs_embeds=None,\n",
    "            decoder_input_ids=None,\n",
    "            decoder_attention_mask=None,\n",
    "            decoder_inputs_embeds=None,\n",
    "            labels=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            task_ids=None,\n",
    "            doc_code_input_ids=None,\n",
    "            doc_code_attention_mask=None,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        peft_config = self.active_peft_config\n",
    "        if not peft_config.is_prompt_learning:\n",
    "            if peft_config.peft_type == PeftType.POLY:\n",
    "                kwargs[\"task_ids\"] = task_ids\n",
    "\n",
    "            with self._enable_peft_forward_hooks(**kwargs):\n",
    "                kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}\n",
    "                return self.base_model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    inputs_embeds=inputs_embeds,\n",
    "                    decoder_input_ids=decoder_input_ids,\n",
    "                    decoder_attention_mask=decoder_attention_mask,\n",
    "                    decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                    labels=labels,\n",
    "                    output_attentions=output_attentions,\n",
    "                    output_hidden_states=output_hidden_states,\n",
    "                    return_dict=return_dict,\n",
    "                    **kwargs,\n",
    "                )\n",
    "\n",
    "        batch_size = _get_batch_size(input_ids, inputs_embeds)\n",
    "        if decoder_attention_mask is not None:\n",
    "            # concat prompt attention mask\n",
    "            prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(\n",
    "                decoder_attention_mask.device\n",
    "            )\n",
    "            if peft_config.peft_type not in [PeftType.PROMPT_TUNING, PeftType.P_TUNING]:\n",
    "                decoder_attention_mask = torch.cat((prefix_attention_mask, decoder_attention_mask), dim=1)\n",
    "\n",
    "        if kwargs.get(\"position_ids\", None) is not None:\n",
    "            warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n",
    "            kwargs[\"position_ids\"] = None\n",
    "        if kwargs.get(\"token_type_ids\", None) is not None:\n",
    "            warnings.warn(\"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\")\n",
    "            kwargs[\"token_type_ids\"] = None\n",
    "        kwargs.update(\n",
    "            {\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"decoder_attention_mask\": decoder_attention_mask,\n",
    "                \"labels\": labels,\n",
    "                \"output_attentions\": output_attentions,\n",
    "                \"output_hidden_states\": output_hidden_states,\n",
    "                \"return_dict\": return_dict,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            labels_ids = labels.clone().to(device)\n",
    "            labels_ids[labels_ids == -100] = self.config.pad_token_id\n",
    "            past_key_values, mu, logvar, prior_mu = self.get_prompt(batch_size,\n",
    "                                                                    input_ids=input_ids,\n",
    "                                                                    attention_mask=attention_mask,\n",
    "                                                                    labels_id=labels_ids,\n",
    "                                                                    labels_attention_mask=decoder_attention_mask,\n",
    "                                                                    doc_code_input_ids=doc_code_input_ids,\n",
    "                                                                    doc_code_attention_mask=doc_code_attention_mask,)\n",
    "            kl_loss = self.kl_loss(mu, logvar, prior_mu, torch.zeros_like(logvar).to(logvar.device))\n",
    "            outputs = self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                past_key_values=past_key_values,\n",
    "                **kwargs,\n",
    "            )\n",
    "            outputs.kl_loss = kl_loss.mean()\n",
    "            return outputs\n",
    "        elif peft_config.peft_type in [PeftType.PROMPT_TUNING, PeftType.P_TUNING]:\n",
    "            if inputs_embeds is None:\n",
    "                inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                # concat prompt attention mask\n",
    "                prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(\n",
    "                    attention_mask.device\n",
    "                )\n",
    "                kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "\n",
    "            prompts = self.get_prompt(batch_size=batch_size)\n",
    "            prompts = prompts.to(inputs_embeds.dtype)\n",
    "            inputs_embeds = torch.cat((prompts[:, : peft_config.num_virtual_tokens], inputs_embeds), dim=1)\n",
    "\n",
    "            return self.base_model(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                **kwargs,\n",
    "            )\n",
    "        else:\n",
    "            if inputs_embeds is None:\n",
    "                inputs_embeds = self.word_embeddings(input_ids)\n",
    "            if decoder_inputs_embeds is None and decoder_input_ids is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "                decoder_inputs_embeds = self.word_embeddings(decoder_input_ids)\n",
    "\n",
    "            if attention_mask is not None:\n",
    "                # concat prompt attention mask\n",
    "                prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(\n",
    "                    attention_mask.device\n",
    "                )\n",
    "                kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, attention_mask), dim=1)\n",
    "            # concat prompt labels\n",
    "            if labels is not None:\n",
    "                if peft_config.num_transformer_submodules == 1:\n",
    "                    kwargs[\"labels\"] = labels\n",
    "                elif peft_config.num_transformer_submodules == 2:\n",
    "                    prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), -100).to(labels.device)\n",
    "                    kwargs[\"labels\"] = torch.cat((prefix_labels, labels), dim=1)\n",
    "            prompts = self.get_prompt(batch_size=batch_size, task_ids=task_ids)\n",
    "            prompts = prompts.to(inputs_embeds.dtype)\n",
    "            inputs_embeds = torch.cat((prompts[:, : peft_config.num_virtual_tokens], inputs_embeds), dim=1)\n",
    "            if peft_config.num_transformer_submodules == 1:\n",
    "                return self.base_model(inputs_embeds=inputs_embeds, **kwargs)\n",
    "            elif peft_config.num_transformer_submodules == 2:\n",
    "                decoder_inputs_embeds = torch.cat(\n",
    "                    (prompts[:, peft_config.num_virtual_tokens :], decoder_inputs_embeds), dim=1\n",
    "                )\n",
    "                return self.base_model(\n",
    "                    inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, **kwargs\n",
    "                )\n",
    "\n",
    "    def generate(self, **kwargs):\n",
    "        peft_config = self.active_peft_config\n",
    "        self.base_model.prepare_inputs_for_generation = self.prepare_inputs_for_generation\n",
    "        self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n",
    "            self._prepare_encoder_decoder_kwargs_for_generation\n",
    "        )\n",
    "        try:\n",
    "            if not peft_config.is_prompt_learning:\n",
    "                with self._enable_peft_forward_hooks(**kwargs):\n",
    "                    kwargs = {k: v for k, v in kwargs.items() if k not in self.special_peft_forward_args}\n",
    "                    outputs = self.base_model.generate(**kwargs)\n",
    "            else:\n",
    "                if \"input_ids\" not in kwargs:\n",
    "                    raise ValueError(\"input_ids must be provided for Peft model generation\")\n",
    "                if kwargs.get(\"position_ids\", None) is not None:\n",
    "                    warnings.warn(\n",
    "                        \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n",
    "                    )\n",
    "                    kwargs[\"position_ids\"] = None\n",
    "                if kwargs.get(\"token_type_ids\", None) is not None:\n",
    "                    warnings.warn(\n",
    "                        \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n",
    "                    )\n",
    "                    kwargs[\"token_type_ids\"] = None\n",
    "\n",
    "                if peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "                    self.curr_input_ids = kwargs.get(\"input_ids\", None)\n",
    "                    self.num_beams = kwargs.get(\"num_beams\", None)\n",
    "                    outputs = self.base_model.generate(**kwargs)\n",
    "                elif peft_config.peft_type in [\n",
    "                    PeftType.PROMPT_TUNING,\n",
    "                    PeftType.P_TUNING,\n",
    "                    PeftType.MULTITASK_PROMPT_TUNING,\n",
    "                ]:\n",
    "                    kwargs = deepcopy(kwargs)\n",
    "\n",
    "                    if \"encoder_outputs\" in kwargs:\n",
    "                        del kwargs[\"encoder_outputs\"]\n",
    "                        warnings.warn(\n",
    "                            \"`encoder_outputs` should not be passed to `generate` when using prompt tuning. Ignoring it.\"\n",
    "                        )\n",
    "\n",
    "                    input_ids = kwargs.pop(\"input_ids\")\n",
    "                    inputs_embeds = self.word_embeddings(input_ids)\n",
    "                    batch_size = inputs_embeds.shape[0]\n",
    "                    prompts = self.get_prompt(batch_size=batch_size, task_ids=kwargs.pop(\"task_ids\", None))\n",
    "                    prompts = prompts.to(inputs_embeds.dtype)\n",
    "\n",
    "                    inputs_embeds = torch.cat((prompts[:, : peft_config.num_virtual_tokens], inputs_embeds), dim=1)\n",
    "                    kwargs[\"inputs_embeds\"] = inputs_embeds\n",
    "\n",
    "                    if \"attention_mask\" in kwargs:\n",
    "                        prefix_attention_mask = torch.ones(batch_size, peft_config.num_virtual_tokens).to(\n",
    "                            kwargs[\"attention_mask\"].device\n",
    "                        )\n",
    "                        kwargs[\"attention_mask\"] = torch.cat((prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1)\n",
    "\n",
    "                    return self.base_model.generate(**kwargs)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "        except:\n",
    "            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n",
    "            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n",
    "                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n",
    "            )\n",
    "            raise\n",
    "        else:\n",
    "            self.base_model.prepare_inputs_for_generation = self.base_model_prepare_inputs_for_generation\n",
    "            self.base_model._prepare_encoder_decoder_kwargs_for_generation = (\n",
    "                self.base_model_prepare_encoder_decoder_kwargs_for_generation\n",
    "            )\n",
    "            return outputs\n",
    "\n",
    "    def prepare_inputs_for_generation(self, *args, task_ids: torch.Tensor = None, **kwargs):\n",
    "        peft_config = self.active_peft_config\n",
    "        model_kwargs = self.base_model_prepare_inputs_for_generation(*args, **kwargs)\n",
    "        if peft_config.peft_type == PeftType.POLY:\n",
    "            model_kwargs[\"task_ids\"] = task_ids\n",
    "        if model_kwargs[\"past_key_values\"] is None and peft_config.peft_type == PeftType.PREFIX_TUNING:\n",
    "            batch_size = model_kwargs[\"decoder_input_ids\"].shape[0]\n",
    "            attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "            assert self.curr_input_ids is not None and attention_mask is not None, \"input_ids and attention_mask must be provided\"\n",
    "            past_key_values, _, _, _ = self.get_prompt(batch_size, std_scale=self.std_scale, input_ids=self.curr_input_ids, attention_mask=attention_mask, num_beams=self.num_beams)\n",
    "            self.curr_input_ids = None\n",
    "            self.num_beams = None\n",
    "            # print(len(past_key_values))\n",
    "            # for i in range(len(past_key_values)):\n",
    "            #     print(past_key_values[i].shape)\n",
    "            model_kwargs[\"past_key_values\"] = past_key_values\n",
    "\n",
    "        return model_kwargs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.095207Z",
     "start_time": "2024-05-31T06:50:12.061502Z"
    }
   },
   "id": "5340bb9dc3e50d9f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model = CVAEPeftModelForSeq2SeqLM(backbone_model, peft_config).to(device)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.271832Z",
     "start_time": "2024-05-31T06:50:12.096337Z"
    }
   },
   "id": "3ed01a40bb14efce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,206,592 || all params: 356,898,690 || trainable%: 6.782482726400593\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# peft_model(input_ids=ex_input_ids.to(device), \n",
    "#            attention_mask=ex_attention_mask.to(device),\n",
    "#            labels=ex_labels.to(device),\n",
    "#            decoder_attention_mask=ex_labels_attention_mask.to(device),\n",
    "#            doc_code_input_ids=ex_docs_codes_input_ids.to(device),\n",
    "#            doc_code_attention_mask=ex_docs_codes_attention_mask.to(device)).loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.275265Z",
     "start_time": "2024-05-31T06:50:12.273072Z"
    }
   },
   "id": "6f5c02e6be6ec477",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# peft_model.eval()\n",
    "# peft_model.generate(input_ids=ex_input_ids.to(device),\n",
    "#                     labels=ex_labels.to(device), \n",
    "#                     max_length=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.278273Z",
     "start_time": "2024-05-31T06:50:12.276333Z"
    }
   },
   "id": "6d9034aa3605cf63",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "peft_lr = 5e-5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.281256Z",
     "start_time": "2024-05-31T06:50:12.279140Z"
    }
   },
   "id": "9eb7f5d3a70e41f6",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(peft_model.parameters(), lr=peft_lr)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_loader) * num_epochs),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.286273Z",
     "start_time": "2024-05-31T06:50:12.282179Z"
    }
   },
   "id": "668e350c20b64524",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.289423Z",
     "start_time": "2024-05-31T06:50:12.287217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ],
   "id": "105d73c13a8c2e1a",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T06:50:12.937735Z",
     "start_time": "2024-05-31T06:50:12.290376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "steps = 0\n",
    "curr_epoch = 0\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(checkpoint_name + \".pt\")]\n",
    "if len(checkpoints) > 0:\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]))\n",
    "    peft_model.prompt_encoder[peft_model.active_adapter].load_state_dict(checkpoint['prefix_encoder_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    steps = checkpoint['steps']\n",
    "    curr_epoch = checkpoint['epoch']\n",
    "    print(\"Loaded checkpoint: \", checkpoints[-1], \" at epoch \", curr_epoch, \" and steps \", steps)\n",
    "    print(f\"ML Loss: {checkpoint['ml_loss']}, KL Loss: {checkpoint['kl_loss']}\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")"
   ],
   "id": "b3ca116e04985fb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint:  codet5p_full_cvae_ft_prefix_tuning_self_attend_cond_code_prior_lang_python_num_vtokens_2.pt  at epoch  189  and steps  699867\n",
      "ML Loss: 0.006632651334499315, KL Loss: 0.8372590363653422\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "# measure model performance before fine-tuning\n",
    "if curr_epoch < num_epochs:\n",
    "    for l in langs:\n",
    "        test_source_dir = \"./data/{}/test/code.original\".format(l)\n",
    "        test_target_dir = \"./data/{}/test/javadoc.original\".format(l)\n",
    "        print(\"Test BLEU for {}: \".format(l), eval_bleu(peft_model,\n",
    "                                                        device,\n",
    "                                                        test_source_dir,\n",
    "                                                        test_target_dir,\n",
    "                                                        tokenizer,\n",
    "                                                        std_scale=1.0))\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71e859cfb0d2055a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, \n",
    "          device, \n",
    "          train_loader, \n",
    "          optimizer, \n",
    "          scheduler, \n",
    "          num_epochs,\n",
    "          steps=0,\n",
    "          curr_epoch=0):\n",
    "    steps = steps\n",
    "    kl_betas = torch.cat((torch.linspace(0, 1, 2500), torch.ones(2500).float())).tolist()\n",
    "    for epoch in range(curr_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_ml_loss = 0\n",
    "        total_kl_loss = 0\n",
    "        total_loss = 0\n",
    "        # Wrap the train_loader with tqdm for a progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Load batch to device\n",
    "            kl_beta = kl_betas[steps % 5000]\n",
    "            tokenized_codes = tokenizer(batch['code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "            tokenized_docs = tokenizer(batch['doc_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "            tokenizeds_docs_codes = tokenizer(batch['doc_code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length+max_target_length)\n",
    "            input_ids = tokenized_codes[\"input_ids\"].to(device)\n",
    "            attention_mask = tokenized_codes[\"attention_mask\"].to(device)\n",
    "            labels = tokenized_docs[\"input_ids\"].clone().to(device)\n",
    "            labels[labels == tokenizer.pad_token_id] = -100\n",
    "            labels_ids = tokenized_docs[\"input_ids\"].to(device)\n",
    "            labels_attention_mask = tokenized_docs[\"attention_mask\"].to(device)\n",
    "            doc_code_input_ids = tokenizeds_docs_codes[\"input_ids\"].to(device)\n",
    "            doc_code_attention_mask = tokenizeds_docs_codes[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=labels,\n",
    "                            decoder_attention_mask=labels_attention_mask,\n",
    "                            doc_code_input_ids=doc_code_input_ids,\n",
    "                            doc_code_attention_mask=doc_code_attention_mask)\n",
    "            ml_loss = outputs.loss\n",
    "            kl_loss = outputs.kl_loss\n",
    "            loss = ml_loss  + kl_beta * kl_loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_ml_loss += ml_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix({'ml_loss': ml_loss.item(), 'kl_loss': kl_loss.item(), 'total_loss': loss.item(),'kl_beta': kl_beta, 'steps': steps})\n",
    "\n",
    "        avg_epoch_ml_loss = total_ml_loss / len(train_loader)\n",
    "        avg_epoch_kl_loss = total_kl_loss / len(train_loader)\n",
    "        avg_epoch_loss = (total_ml_loss + total_kl_loss) / len(train_loader)\n",
    "        # evaluate model performance every 5 epochs\n",
    "\n",
    "        print(f\"Epoch {epoch+1} completed. Average ML Loss: {avg_epoch_ml_loss}, Average KL Loss: {avg_epoch_kl_loss}, Average Loss: {avg_epoch_loss}\")\n",
    "        \n",
    "        # model.save_pretrained(checkpoint_name + \"peft_params\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'prefix_encoder_state_dict': model.prompt_encoder[model.active_adapter].state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'steps': steps,\n",
    "            'ml_loss': avg_epoch_ml_loss,\n",
    "            'kl_loss': avg_epoch_kl_loss,\n",
    "        }, os.path.join(checkpoint_dir, checkpoint_name+\".pt\"))\n",
    "        \n",
    "        if (epoch+1) % 5 == 0:\n",
    "            for l in langs:\n",
    "                validation_source_dir = \"./data/{}/dev/code.original\".format(l)\n",
    "                validation_target_dir = \"./data/{}/dev/javadoc.original\".format(l)\n",
    "                print(\"Validation BLEU for {}: \".format(l),\n",
    "                      eval_bleu(model,\n",
    "                                device,\n",
    "                                validation_source_dir,\n",
    "                                validation_target_dir,\n",
    "                                tokenizer,\n",
    "                                std_scale=1.0))\n",
    "                "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T06:52:38.405961Z",
     "start_time": "2024-05-31T06:52:38.395170Z"
    }
   },
   "id": "39822520cbb23be2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "train(peft_model, \n",
    "      device, \n",
    "      train_loader, \n",
    "      optimizer,\n",
    "      scheduler, \n",
    "      num_epochs,\n",
    "      steps=steps,\n",
    "      curr_epoch=curr_epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde269282f97a59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# measure model performance before fine-tuning\n",
    "if curr_epoch < num_epochs:\n",
    "    for l in langs:\n",
    "        test_source_dir = \"./data/{}/test/code.original\".format(l)\n",
    "        test_target_dir = \"./data/{}/test/javadoc.original\".format(l)\n",
    "        print(\"Test BLEU for {}: \".format(l), eval_bleu(peft_model,\n",
    "                                                        device,\n",
    "                                                        test_source_dir,\n",
    "                                                        test_target_dir,\n",
    "                                                        tokenizer,\n",
    "                                                        std_scale=1.0))\n",
    "        torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69e42173100fce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "# model.save_pretrained(\"codet5p_ft_epoch_{}_lang_{}\".format(num_epochs, lang))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T08:50:12.384764Z",
     "start_time": "2024-05-31T08:50:12.382361Z"
    }
   },
   "id": "b1eff1055b378dfd",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "def distinct_with_beam_search(model,\n",
    "                              device,\n",
    "                              src_dir,\n",
    "                              tgt_dir,\n",
    "                              tokenizer,\n",
    "                              batch_size=16,\n",
    "                              beam_size=10,\n",
    "                              num_return_sequences=8,\n",
    "                              std_scale=1.0):\n",
    "    model.eval()\n",
    "    model.std_scale = std_scale\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    all_summaries = []\n",
    "    eval_inputs = {\n",
    "        \"code_text\": source_codes,\n",
    "        \"doc_text\": targets\n",
    "    }\n",
    "    eval_data = Dataset.from_dict(eval_inputs)\n",
    "    eval_data.set_format(type='torch', columns=['code_text', 'doc_text'])\n",
    "    eval_loader = DataLoader(eval_data, batch_size=batch_size)\n",
    "    for batch in tqdm(eval_loader):\n",
    "        tokenized_codes = tokenizer(batch['code_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_length)\n",
    "        tokenized_docs = tokenizer(batch['doc_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_target_length)\n",
    "        input_ids = tokenized_codes[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized_codes[\"attention_mask\"].to(device)\n",
    "        labels = tokenized_docs[\"input_ids\"].clone().to(device)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        labels_ids = tokenized_docs[\"input_ids\"].to(device)\n",
    "        labels_attention_mask = tokenized_docs[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids=input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=100, \n",
    "                                       num_beams=beam_size, \n",
    "                                       num_return_sequences=num_return_sequences)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in all_summaries]))\n",
    "    # repeat targets for each generated sequence\n",
    "    repeated_targets = []\n",
    "    for target in targets:\n",
    "        repeated_targets.extend([target]*num_return_sequences)\n",
    "    references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "    #calculate oracle scores\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    reshaped_bleu = np.array(list(ind_bleu.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_bleu = np.max(reshaped_bleu, axis=1)\n",
    "    rouge_calculator = Rouge()\n",
    "    rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_rouge = np.array(list(ind_rouge.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_rouge = np.max(reshaped_rouge, axis=1)\n",
    "    meteor_calculator = Meteor()\n",
    "    meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_meteor = np.array(list(ind_meteor)).reshape(-1, num_return_sequences)\n",
    "    oracle_meteor = np.max(reshaped_meteor, axis=1)\n",
    "    print(\"Oracle_bleu: {}, Oracle_rouge: {}, Oracle_meteor: {}\".format(np.mean(oracle_bleu) * 100,\n",
    "                                                                         np.mean(oracle_rouge) * 100,\n",
    "                                                                         np.mean(oracle_meteor) * 100))\n",
    "\n",
    "    return hypotheses, references"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T08:50:12.395576Z",
     "start_time": "2024-05-31T08:50:12.385721Z"
    }
   },
   "id": "47c3e61c2fd3e6b0",
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "# num_return_sequences_list = [4, 8, 12, 16, 20]\n",
    "# for num_return_sequences in num_return_sequences_list:\n",
    "#     for l in langs:\n",
    "#         test_source_dir = \"./data/{}/test/code.original\".format(l)\n",
    "#         test_target_dir = \"./data/{}/test/javadoc.original\".format(l)\n",
    "#         print(\"Test distinct with beam search for {} with beam size {}: \".format(l, num_return_sequences))\n",
    "#         hypotheses, references = distinct_with_beam_search(peft_model,\n",
    "#                                                            device,\n",
    "#                                                            test_source_dir,\n",
    "#                                                            test_target_dir,\n",
    "#                                                            tokenizer,\n",
    "#                                                            batch_size=4,\n",
    "#                                                            beam_size=num_return_sequences,\n",
    "#                                                            num_return_sequences=num_return_sequences,\n",
    "#                                                            std_scale=1.0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-31T08:50:12.399027Z",
     "start_time": "2024-05-31T08:50:12.396499Z"
    }
   },
   "id": "ed74cdbdc45bd27e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T08:50:12.403132Z",
     "start_time": "2024-05-31T08:50:12.400058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, hypotheses, references):\n",
    "        self.hypotheses = list(chain.from_iterable(hypotheses.values()))\n",
    "        self.references = list(chain.from_iterable(references.values()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hypotheses[idx], self.references[idx]"
   ],
   "id": "60a724faaa8b696f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-31T08:50:12.422877Z",
     "start_time": "2024-05-31T08:50:12.404156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def distinct_with_latent(model,\n",
    "                         device,\n",
    "                         src_dir,\n",
    "                         tgt_dir,\n",
    "                         tokenizer,\n",
    "                         std_scale=20,\n",
    "                         batch_size=1,\n",
    "                         num_latents=100,\n",
    "                         num_beams=4):\n",
    "    model.eval()\n",
    "    model.std_scale = std_scale\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    all_summaries = []\n",
    "    # repeat each code num_latents times\n",
    "    repeated_source_codes = []\n",
    "\n",
    "    for code in source_codes:\n",
    "        repeated_source_codes.extend([code]*num_latents)\n",
    "    for i in tqdm(range(0, len(repeated_source_codes), batch_size)):\n",
    "        batch = repeated_source_codes[i:i+batch_size]\n",
    "        input = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=400)\n",
    "        input_ids = input[\"input_ids\"].to(device)\n",
    "        attention_mask = input[\"attention_mask\"].to(device)\n",
    "        # out = model.encoder(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     labels_id=None,\n",
    "        #                     labels_attention_mask=None,\n",
    "        #                     num_prefix=num_prefix,\n",
    "        #                     std_scale=std_scale,\n",
    "        #                     return_dict=True)[0]\n",
    "        # enc_out = BaseModelOutput(last_hidden_state=out)\n",
    "        # generated_ids = model.decoder.generate(encoder_outputs=enc_out, max_length=80, num_beams=1)\n",
    "        if num_beams <= 1:\n",
    "            generated_ids = model.generate(input_ids=input_ids,\n",
    "                                           attention_mask=attention_mask,\n",
    "                                           max_length=80,\n",
    "                                           )\n",
    "        else:\n",
    "            generated_ids = model.generate(input_ids=input_ids,\n",
    "                                           attention_mask=attention_mask,\n",
    "                                           max_length=80,\n",
    "                                           num_beams=num_beams,\n",
    "                                           num_return_sequences=1)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "\n",
    "    # separate the summaries into nested list of num_latents\n",
    "    separated_summaries = [all_summaries[i:i+num_latents] for i in range(0, len(all_summaries), num_latents)]\n",
    "    all_summaries_dict = [[source_codes[i],\n",
    "                           re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", targets[i].strip().lower()),\n",
    "                           [re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .') for summary in separated_summaries[i]]]\n",
    "                          for i in range(len(source_codes))]\n",
    "    all_ref_fw = open(os.path.join(\"T5_results\", \"generated\", lang, f\"num_latents_{num_latents}_std_scale_{std_scale}_beam_{num_beams}_hypotheses_all.json\"), 'w')\n",
    "    \n",
    "    # raw summaries of 100 is now generated\n",
    "    json.dump(all_summaries_dict, all_ref_fw, indent=4)\n",
    "    all_ref_fw.close()\n",
    "    return all_summaries_dict"
   ],
   "id": "4c20f32a62fc5918",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# generate distinct set with latent sampling, you should tune the std_scales based on your dataset\n",
    "std_scales = [5]\n",
    "for std_scale in std_scales:\n",
    "    print(\"current std_scale: \", std_scale)\n",
    "    for l in langs:\n",
    "        # change test dataset dir here\n",
    "        test_source_dir = \"./data/{}/test/code.original\".format(l)\n",
    "        test_target_dir = \"./data/{}/test/javadoc.original\".format(l)\n",
    "        all_summaries_dict = distinct_with_latent(peft_model,\n",
    "                                             device,\n",
    "                                             test_source_dir,\n",
    "                                             test_target_dir,\n",
    "                                             tokenizer,\n",
    "                                             std_scale=std_scale,\n",
    "                                             batch_size=100,\n",
    "                                             num_latents=100,\n",
    "                                             num_beams=4)"
   ],
   "id": "90112c3ac5a45cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compute model score for output summary, recalculated since the log probs are perturbed by latent variables\n",
    "def score_model_sequence_scoring(code, all_candidates, score_model, score_tokenizer, batch_size):\n",
    "\n",
    "    tokenized_code = score_tokenizer([code] * len(all_candidates), return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    tokenized_candidates = score_tokenizer(all_candidates, return_tensors=\"pt\", padding=True).to(device)\n",
    "    tokenized_candidates_cat_bos_token = torch.cat([torch.zeros(tokenized_candidates.input_ids.shape[0], 1, dtype=torch.long).to(device), tokenized_candidates.input_ids], dim=-1)\n",
    "    with torch.no_grad():\n",
    "        scores = []\n",
    "        for i in range(0, len(all_candidates), batch_size):\n",
    "            batch_code_tokens = tokenized_code.input_ids[i:i+batch_size]\n",
    "            batch_hyp_tokens = tokenized_candidates_cat_bos_token[i:i+batch_size]\n",
    "            batch_decode_outputs = score_model(input_ids=batch_code_tokens, decoder_input_ids=batch_hyp_tokens)\n",
    "            batch_decode_outputs_logits_tuple = [batch_decode_outputs.logits[:, j, :] for j in range(batch_decode_outputs.logits.shape[1]-1)]\n",
    "            batch_decode_transition_scores = score_model.compute_transition_scores(batch_hyp_tokens[:, 1:], batch_decode_outputs_logits_tuple, normalize_logits=True)\n",
    "            batch_decode_transition_scores_masked = batch_decode_transition_scores * tokenized_candidates.attention_mask[i:i+batch_size]\n",
    "            batch_decode_sequence_score = batch_decode_transition_scores_masked[:, 1:].sum(dim=-1)/tokenized_candidates.attention_mask[i:i+batch_size, 1:].sum(dim=-1)\n",
    "            scores.extend(batch_decode_sequence_score.cpu().detach().numpy().tolist())\n",
    "    scores = [(candidate, score) for candidate, score in zip(all_candidates, scores)]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores"
   ],
   "id": "8933a4723ba78cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_all_sequence_scores(summaries, score_model, score_tokenizer, batch_size, num_latents, std_scale, num_beams):\n",
    "    all_sequence_scores = []\n",
    "    for source_code, target, predicted in tqdm(summaries):\n",
    "        all_sequence_scores.append([source_code,\n",
    "                                    target,\n",
    "                                    score_model_sequence_scoring(source_code, predicted, score_model, score_tokenizer, batch_size)])\n",
    "    all_ref_fw = open(os.path.join(\"T5_results\", \"generated\", lang, f\"num_latents_{num_latents}_std_scale_{std_scale}_beam_{num_beams}_hypotheses_with_seq_score_all.json\"), 'w')\n",
    "    json.dump(all_sequence_scores, all_ref_fw, indent=4)\n",
    "    all_ref_fw.close()\n",
    "    return all_sequence_scores"
   ],
   "id": "627945167e9f9e7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_sequence_scores = calculate_all_sequence_scores(all_summaries_dict, backbone_model, tokenizer, batch_size=100, num_latents=100, std_scale=5, num_beams=4)",
   "id": "a7f088cea5dd7e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_pairwise_bleu(summaries):\n",
    "    distinct_summaries = set(summaries)\n",
    "    distinct_summaries_list = list(distinct_summaries)\n",
    "    # assign a unique id to each distinct summary\n",
    "    summary_to_id = {summary: i for i, summary in enumerate(distinct_summaries_list)}\n",
    "    # compute pairwise BLEU scores\n",
    "    distinct_pairwise_bleu_scores = np.zeros((len(distinct_summaries_list), len(distinct_summaries_list)))\n",
    "    for i, summary_i in enumerate(distinct_summaries_list):\n",
    "        distinct_pairwise_bleu_scores[i, i] = 1.0\n",
    "        for j, summary_j in enumerate(distinct_summaries_list[i + 1:]):\n",
    "            ref = [summary_i.split()]\n",
    "            hyp = summary_j.split()\n",
    "            distinct_pairwise_bleu_scores[i, j+i+1] = compute_bleu([ref], [hyp], smooth=True)[0]\n",
    "            distinct_pairwise_bleu_scores[j+i+1, i] = distinct_pairwise_bleu_scores[i, j+i+1]\n",
    "\n",
    "    pairwise_bleu = np.zeros((len(summaries), len(summaries)))\n",
    "    pairwise_bleu += np.eye(len(summaries)) * 1.0\n",
    "    for i, summary_i in enumerate(summaries):\n",
    "        for j, summary_j in enumerate(summaries[i+1:]):\n",
    "            pairwise_bleu[i, j+i+1] = distinct_pairwise_bleu_scores[summary_to_id[summary_i], summary_to_id[summary_j]]\n",
    "            pairwise_bleu[j+i+1, i] = pairwise_bleu[i, j+i+1]\n",
    "\n",
    "    return distinct_pairwise_bleu_scores, summary_to_id, pairwise_bleu\n",
    "    "
   ],
   "id": "2a05ad2ca2cd8643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pairwise_diversity_scores = []\n",
    "for source_code, target, predicted_and_score in tqdm(all_sequence_scores):\n",
    "    predicted = [item[0] for item in predicted_and_score]\n",
    "    pairwise_diversity_scores.append(compute_pairwise_bleu(predicted))"
   ],
   "id": "d08371acd5de5f96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import didppy as dp\n",
    "# bi-criteria subset selection\n",
    "def find_optimal_pair(h_arr, g_arr, alpha=1, beta=1, count=1, return_set=True, return_score=False):\n",
    "    if count < 0:\n",
    "        raise ValueError(\"Count must be non-negative\")\n",
    "    arr = np.zeros_like(h_arr)\n",
    "    if alpha == 0 and beta == 0:\n",
    "        raise ValueError('This does not make sense! Alpha and Beta are both zeros.')\n",
    "    if beta != 0:\n",
    "        arr += beta*h_arr\n",
    "    if alpha != 0:\n",
    "        g_arr_i = g_arr[:, np.newaxis]\n",
    "        g_arr_j = g_arr[np.newaxis, :]\n",
    "        arr += alpha*g_arr_i + alpha*g_arr_j\n",
    "    # Ensure the array is 2D and symmetric\n",
    "    if arr.ndim != 2 or not np.allclose(arr, arr.T):\n",
    "        raise ValueError(f\"Array must be 2D and symmetric, but array is: {arr}\")\n",
    "\n",
    "    # Create a mask for the upper triangle, including the diagonal\n",
    "    upper_triangle_mask = np.triu(np.ones_like(arr, dtype=bool))\n",
    "\n",
    "    # Apply mask and find unique values in the lower triangle\n",
    "    masked_arr = np.ma.masked_array(arr, mask=upper_triangle_mask)\n",
    "    unique_values = np.unique(masked_arr.compressed())[::-1]  # Sorted in descending order\n",
    "    max_indices = []\n",
    "    i = 0  # Index to iterate over unique values\n",
    "    while len(max_indices) < count and i < len(unique_values):\n",
    "        # Find positions of the current max value\n",
    "        current_max_value = unique_values[i]\n",
    "        current_positions = np.ma.where(masked_arr == current_max_value)\n",
    "        # Combine the row and column indices into tuples\n",
    "        current_max_indices = [{i, j} if return_set else [i, j] for i, j in zip(current_positions[0],\n",
    "                                                                                current_positions[1])]\n",
    "        # Ensure the indices are unique\n",
    "        if return_score:\n",
    "            max_indices.extend([(current_max_value, idx) for idx in current_max_indices])\n",
    "        else:\n",
    "            max_indices.extend(current_max_indices)\n",
    "        i += 1\n",
    "\n",
    "    # Ensure we do not exceed the count\n",
    "    if count == 1:\n",
    "        max_indices = max_indices[0]\n",
    "    else:\n",
    "        max_indices = max_indices[:count]\n",
    "    return max_indices\n",
    "\n",
    "def HDBS(diversity_matrix, cost_dict, summary_to_id, alpha=1, beta=1, criteria='sum',\n",
    "         k=5, beam_width=256, seed=42, threads=64, initialize=False, parallelization='hdbs1'):\n",
    "    if len(summary_to_id) <= k:\n",
    "        ids = []\n",
    "        scores = []\n",
    "        for summary in summary_to_id:\n",
    "            ids.append(summary_to_id[summary])\n",
    "            scores.append(cost_dict[summary])\n",
    "        return ids, scores\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    diversity_matrix = 1 - diversity_matrix\n",
    "    cost_vector = np.zeros(len(cost_dict))\n",
    "    for summary in cost_dict:\n",
    "        cost_vector[summary_to_id[summary]] = cost_dict[summary]\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    model = dp.Model(maximize=True, float_cost=True)\n",
    "    n = diversity_matrix.shape[0]\n",
    "\n",
    "    selection = model.add_object_type(number=n)\n",
    "    selected = model.add_set_var(object_type=selection, target=[], name=\"selected\")\n",
    "\n",
    "    # if criteria == 'sum':\n",
    "    #     diversity_matrix = diversity_matrix*1000000\n",
    "    # else:\n",
    "    #     diversity_matrix = (diversity_matrix +\n",
    "    #                         np.diag(np.diag(diversity_matrix + 1))) * 1000000\n",
    "    # \n",
    "    # cost_vector = cost_vector*1000000\n",
    "    cst = model.add_float_table(cost_vector)\n",
    "    div = model.add_float_table(diversity_matrix)\n",
    "\n",
    "    if criteria == 'sum':\n",
    "        if initialize:\n",
    "            p1, p2 = find_optimal_pair(diversity_matrix,\n",
    "                                       cost_vector,\n",
    "                                       alpha=alpha, beta=beta)\n",
    "            first_selection = True\n",
    "            for j in range(n):\n",
    "                if j == p1 or j == p2 and first_selection:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=beta*div[j, selected] + alpha*cst[j] + dp.FloatExpr.state_cost(),\n",
    "                        effects=[\n",
    "                            (selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select, forced=True)\n",
    "                    first_selection = False\n",
    "                else:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=beta*div[j, selected] + alpha*cst[j] + dp.FloatExpr.state_cost(),\n",
    "                        effects=[\n",
    "                            (selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select)\n",
    "        else:\n",
    "            for j in range(n):\n",
    "                select = dp.Transition(\n",
    "                    name=f\"select_{j}\",\n",
    "                    cost=beta*div[j, selected] + alpha*cst[j] + dp.FloatExpr.state_cost(),\n",
    "                    effects=[\n",
    "                        (selected, selected.add(j))],\n",
    "                    preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                )\n",
    "                model.add_transition(select)\n",
    "    elif criteria == 'min':\n",
    "        if beam_width == 1:\n",
    "            p1, p2 = find_optimal_pair(diversity_matrix,\n",
    "                                       cost_vector,\n",
    "                                       alpha=alpha, beta=beta)\n",
    "            for j in range(n):\n",
    "                if j == p1 or j == p2:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=dp.min(alpha*100000+beta*100000,\n",
    "                                    dp.FloatExpr.state_cost()+alpha*100000+beta*100000),\n",
    "                        effects=[(selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select, forced=True)\n",
    "                else:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=dp.min(alpha*dp.min(cst.min(selected), cst[j]) + beta*dp.min(div.min(selected, selected),\n",
    "                                                                                          div.min(j, selected)),\n",
    "                                    dp.FloatExpr.state_cost()),\n",
    "                        effects=[(selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select)\n",
    "        elif initialize:\n",
    "            p1, p2 = find_optimal_pair(diversity_matrix,\n",
    "                                       cost_vector,\n",
    "                                       alpha=alpha, beta=beta)\n",
    "            first_selection = True\n",
    "            for j in range(n):\n",
    "                if j == p1 or j == p2 and first_selection:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=dp.min(alpha*100000+beta*100000,\n",
    "                                    dp.FloatExpr.state_cost()+alpha*100000+beta*100000),\n",
    "                        effects=[(selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select, forced=True)\n",
    "                    first_selection = False\n",
    "                else:\n",
    "                    select = dp.Transition(\n",
    "                        name=f\"select_{j}\",\n",
    "                        cost=dp.min(alpha*dp.min(cst.min(selected), cst[j]) + beta*dp.min(div.min(selected, selected),\n",
    "                                                                                          div.min(j, selected)),\n",
    "                                    dp.FloatExpr.state_cost()),\n",
    "                        effects=[(selected, selected.add(j))],\n",
    "                        preconditions=[selected.len() < k, selected.complement().contains(j)],\n",
    "                    )\n",
    "                    model.add_transition(select)\n",
    "        else:\n",
    "            for j in range(n):\n",
    "                select_first = dp.Transition(\n",
    "                    name=f\"selectf_{j}\",\n",
    "                    cost=dp.min(alpha*100000+beta*100000,\n",
    "                                dp.FloatExpr.state_cost()+alpha*100000+beta*100000),\n",
    "                    effects=[(selected, selected.add(j))],\n",
    "                    preconditions=[selected.len() == 0, selected.complement().contains(j)],\n",
    "                )\n",
    "                model.add_transition(select_first)\n",
    "                select = dp.Transition(\n",
    "                    name=f\"select_{j}\",\n",
    "                    cost=dp.min(alpha*dp.min(cst.min(selected), cst[j]) + beta*dp.min(div.min(selected, selected),\n",
    "                                                                                      div.min(j, selected)),\n",
    "                                dp.FloatExpr.state_cost()),\n",
    "                    effects=[(selected, selected.add(j))],\n",
    "                    preconditions=[selected.len() < k, selected.len() > 0, selected.complement().contains(j)],\n",
    "                )\n",
    "                model.add_transition(select)\n",
    "\n",
    "    model.add_base_case([selected.len() == k])\n",
    "    if parallelization == 'hdbs1':\n",
    "        method = dp.BeamParallelizationMethod.Hdbs1\n",
    "    elif parallelization == 'hdbs2':\n",
    "        method = dp.BeamParallelizationMethod.Hdbs2\n",
    "    else:\n",
    "        method = dp.BeamParallelizationMethod.Sbs\n",
    "    solver = dp.CABS(model,\n",
    "                     parallelization_method=method,\n",
    "                     initial_beam_size=beam_width,\n",
    "                     max_beam_size=beam_width,\n",
    "                     threads=threads,\n",
    "                     quiet=True,\n",
    "                     time_limit=1)\n",
    "    solution = solver.search()\n",
    "    # print([t for t in solution.transitions])\n",
    "    beam_result = [int(t.name.split('_')[1]) for t in solution.transitions]\n",
    "    return beam_result, solution.cost/100000"
   ],
   "id": "ac97a83cd4e29230"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def bicriteria_subset_selection(summaries, num_distinct_summary_list, std_scale, pairwise_diversity_scores, alpha=1, beta=1):\n",
    "    # sequence_scores = []\n",
    "\n",
    "    for num_distinct_summary in num_distinct_summary_list:\n",
    "        # keep at most num_distinct_summary distinct summaries\n",
    "        distinct_summaries = []\n",
    "        distinct_guesses = []\n",
    "        repeated_targets = []\n",
    "        separated_hypotheses = []\n",
    "        for i, summaries_lst in tqdm(enumerate(summaries)):\n",
    "            source_code, target, model_scores = summaries_lst\n",
    "            # predicted = [item[0] for item in predicted_and_score]\n",
    "            # sequence_scores = [item[1] for item in predicted_and_score]\n",
    "            distinct_div_matrix, summary_to_id, _ = pairwise_diversity_scores[i]\n",
    "\n",
    "            # quality_vec = all_codeT5_scores[i]\n",
    "            # distinct_sum = list(set(predicted))\n",
    "            # model_scores = sequence_scores[i]\n",
    "            model_scores = [(summary, score) for summary, score in model_scores]\n",
    "            model_scores_distinct = list(set(model_scores))\n",
    "            model_scores_distinct.sort(key=lambda x: x[1], reverse=True)\n",
    "            if len(model_scores_distinct) < num_distinct_summary:\n",
    "                distinct_sum = [summary for summary, _ in model_scores_distinct]\n",
    "            else:\n",
    "                quality_vec = {summary: score for summary, score in model_scores}\n",
    "                id_to_summary = {i: summary for summary, i in summary_to_id.items()}\n",
    "                ids_to_take = HDBS(distinct_div_matrix, quality_vec, summary_to_id, alpha=alpha, beta=beta, k=num_distinct_summary, beam_width=256, seed=42)[0]\n",
    "                distinct_sum = [id_to_summary[id] for id in ids_to_take]\n",
    "            distinct_guesses.append(len(distinct_sum))\n",
    "            separated_hypotheses.append([summary.strip() for summary in distinct_sum])\n",
    "            distinct_summaries.extend(distinct_sum)\n",
    "            repeated_targets.extend([target]*len(distinct_sum))\n",
    "        all_distinct_unigrams_ratio = [distinct_n_corpus_level(preds, n=1) for preds in separated_hypotheses]\n",
    "        all_distinct_bigrams_ratio = [distinct_n_corpus_level(preds, n=2) for preds in separated_hypotheses]\n",
    "        all_self_bleu = [self_bleu_score(preds) for preds in separated_hypotheses]\n",
    "        average_distinct_unigrams_ratio = np.mean(all_distinct_unigrams_ratio)\n",
    "        average_distinct_bigrams_ratio = np.mean(all_distinct_bigrams_ratio)\n",
    "        average_self_bleu = np.mean(all_self_bleu)\n",
    "        hypotheses = dict(enumerate([[summary.strip().lower()] for summary in distinct_summaries]))\n",
    "        references = dict(enumerate([[target.strip().lower()] for target in repeated_targets]))\n",
    "        #calculate oracle scores\n",
    "        print(\"average distinct guesses: \", np.mean(distinct_guesses))\n",
    "        _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "        ind_bleu_input = iter(list(ind_bleu.values()))\n",
    "        sliced_bleu = [list(islice(ind_bleu_input, elem))\n",
    "                       for elem in distinct_guesses]\n",
    "        np_sliced_bleu = np.array(list(zip_longest(*sliced_bleu, fillvalue=0))).T\n",
    "        oracle_bleu = np.max(np_sliced_bleu, axis=1)\n",
    "\n",
    "        rouge_calculator = Rouge()\n",
    "        rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "        ind_rouge_input = iter(list(ind_rouge.values()))\n",
    "        sliced_rouge = [list(islice(ind_rouge_input, elem))\n",
    "                        for elem in distinct_guesses]\n",
    "        np_sliced_rouge = np.array(list(zip_longest(*sliced_rouge, fillvalue=0))).T\n",
    "        oracle_rouge = np.max(np_sliced_rouge, axis=1)\n",
    "\n",
    "\n",
    "        if len(summaries) > 1500:\n",
    "            meteor_calculator = Meteor()\n",
    "            meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "            ind_meteor_input = iter(list(ind_meteor))\n",
    "            sliced_meteor = [list(islice(ind_meteor_input, elem))\n",
    "                             for elem in distinct_guesses]\n",
    "            np_sliced_meteor = np.array(list(zip_longest(*sliced_meteor, fillvalue=0))).T\n",
    "            oracle_meteor = np.max(np_sliced_meteor, axis=1)\n",
    "        else:\n",
    "            oracle_meteor = 0.0\n",
    "        # print(\"Oracle scores, bleu: \", np.mean(oracle_bleu) * 100, \" rouge-l: \", np.mean(oracle_rouge) * 100, \" meteor: \", np.mean(oracle_meteor) * 100)\n",
    "        # write scores to file\n",
    "\n",
    "        bertscore = load(\"bertscore\")\n",
    "\n",
    "        bert_precision = []\n",
    "        bert_recall = []\n",
    "        bert_f1 = []\n",
    "        # Define your batch size\n",
    "        batch_size = 5120\n",
    "        # Create an instance of your dataset\n",
    "        dataset = TextDataset(hypotheses, references)\n",
    "        # Create a DataLoader instance\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        # Iterating over batches and compute BERTScore for each with a progress bar\n",
    "        for hypotheses_batch, references_batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            batch_results = bertscore.compute(predictions=hypotheses_batch,\n",
    "                                              references=references_batch,\n",
    "                                              lang=\"en\")\n",
    "            bert_precision.extend(list(batch_results['precision']))\n",
    "            bert_recall.extend(list(batch_results['recall']))\n",
    "            bert_f1.extend(list(batch_results['f1']))\n",
    "\n",
    "        bert_precision_input = iter(bert_precision)\n",
    "        sliced_bert_precision = [list(islice(bert_precision_input, elem))\n",
    "                                 for elem in distinct_guesses]\n",
    "        np_sliced_bert_precision = np.array(list(zip_longest(*sliced_bert_precision, fillvalue=0))).T\n",
    "        best_bert_precision = np.max(np_sliced_bert_precision, axis=1)\n",
    "\n",
    "        bert_recall_input = iter(bert_recall)\n",
    "        sliced_bert_recall = [list(islice(bert_recall_input, elem))\n",
    "                              for elem in distinct_guesses]\n",
    "        np_sliced_bert_recall = np.array(list(zip_longest(*sliced_bert_recall, fillvalue=0))).T\n",
    "        best_bert_recall = np.max(np_sliced_bert_recall, axis=1)\n",
    "\n",
    "        bert_f1_input = iter(bert_f1)\n",
    "        sliced_bert_f1 = [list(islice(bert_f1_input, elem))\n",
    "                          for elem in distinct_guesses]\n",
    "        np_sliced_bert_f1 = np.array(list(zip_longest(*sliced_bert_f1, fillvalue=0))).T\n",
    "        best_bert_f1 = np.max(np_sliced_bert_f1, axis=1)\n",
    "\n",
    "        with open(os.path.join(\"T5_results\", f\"CodeT5+_VPT_bicriteria_subset_selection_results_{lang}.txt\"), 'a') as f:\n",
    "            f.write(f\"num_distinct_summary: {num_distinct_summary} | std_scale: {std_scale:.1f} | alpha: {alpha} | beta: {beta} | average_distinct_guesses: {np.mean(distinct_guesses): .4f} | average_distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f} | average_distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f} | average_self_bleu: {average_self_bleu * 100: .4f} | Oracle_bleu: {np.mean(oracle_bleu) * 100: .4f} | Oracle_rouge-l: {np.mean(oracle_rouge) * 100: .4f} | Oracle_meteor: {np.mean(oracle_meteor) * 100: .4f} | Oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f} | Oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f} | Oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n",
    "        print(f\"num_distinct_summary: {num_distinct_summary} | std_scale: {std_scale:.1f} | alpha: {alpha} | beta: {beta} | average_distinct_guesses: {np.mean(distinct_guesses): .4f} | average_distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f} | average_distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f} | average_self_bleu: {average_self_bleu * 100: .4f} | Oracle_bleu: {np.mean(oracle_bleu) * 100: .4f} | Oracle_rouge-l: {np.mean(oracle_rouge) * 100: .4f} | Oracle_meteor: {np.mean(oracle_meteor) * 100: .4f} | Oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f} | Oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f} | Oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n"
   ],
   "id": "5619a824b6bd3ebc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# alpha and beta should be tuned for your dataset\n",
    "bicriteria_subset_selection(all_sequence_scores, [10, 20], 5, pairwise_diversity_scores, alpha=1.0, beta=0.5)"
   ],
   "id": "c19248f3f112d3a2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
