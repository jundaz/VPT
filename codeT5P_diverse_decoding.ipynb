{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:22.436421Z",
     "start_time": "2024-05-29T17:23:22.428354Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from itertools import islice, zip_longest\n",
    "from c2nl.eval.bleu import corpus_bleu\n",
    "from c2nl.eval.rouge import Rouge\n",
    "from c2nl.eval.meteor import Meteor\n",
    "from c2nl.eval.distinct_n.distinct_ngrams import distinct_n_corpus_level\n",
    "from c2nl.eval.self_bleu import self_bleu_score\n",
    "from c2nl.eval.self_bert import self_bert_score\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from itertools import islice, zip_longest, chain\n",
    "from evaluate import load\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer, get_linear_schedule_with_warmup, T5ForConditionalGeneration, LogitsProcessorList, BeamSearchScorer\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import gumbel as sbsutils\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:25.333990Z",
     "start_time": "2024-05-29T17:23:22.438187Z"
    }
   },
   "id": "f4508f0b329b2a45",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value)  # Numpy module\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    torch.cuda.manual_seed(seed_value)  # PyTorch CUDA\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch CUDA (for multi-GPU setups)\n",
    "    torch.backends.cudnn.deterministic = True  # For CUDA backend\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # For Python hash seeding\n",
    "# Example usage\n",
    "set_seed(42)  # Replace 42 with your desired seed"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:25.342013Z",
     "start_time": "2024-05-29T17:23:25.335726Z"
    }
   },
   "id": "dd2b93a6082948c2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "lang = \"python\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "torch.cuda.get_device_name(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:25.425639Z",
     "start_time": "2024-05-29T17:23:25.342965Z"
    }
   },
   "id": "ec88e3809a22eb26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 4090'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "if lang == 'java':\n",
    "    base_model ='codet5p_ft_lang_java_backbone'\n",
    "else:\n",
    "    base_model = 'codet5p_ft_lang_python_backbone'\n",
    "if lang == 'java':\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m'\n",
    "else:\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m-bimodal'\n",
    "if 'bimodal' in base_model or 'python' in base_model:\n",
    "    print(\"using auto model\")\n",
    "    model = AutoModel.from_pretrained(base_model, trust_remote_code=True).to(device)\n",
    "else:\n",
    "    print(\"using t5 conditional generation model\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(base_model, trust_remote_code=True).to(device)\n",
    "max_input_length = 512\n",
    "max_target_length = 100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:26.638099Z",
     "start_time": "2024-05-29T17:23:25.427259Z"
    }
   },
   "id": "4fda6d29cc42617b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using auto model\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_tokenizer)\n",
    "train_source_dir = \"./data/{}/train/code.original\".format(lang)\n",
    "train_target_dir = \"./data/{}/train/javadoc.original\".format(lang)\n",
    "validation_source_dir = \"./data/{}/dev/code.original\".format(lang)\n",
    "validation_target_dir = \"./data/{}/dev/javadoc.original\".format(lang)\n",
    "test_source_dir = \"./data/{}/test/code.original\".format(lang)\n",
    "test_target_dir = \"./data/{}/test/javadoc.original\".format(lang)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:26.719584Z",
     "start_time": "2024-05-29T17:23:26.639270Z"
    }
   },
   "id": "d09a30a9e05339e1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "# load my own data\n",
    "\n",
    "codes = open(train_source_dir, 'r').readlines()\n",
    "docs = open(train_target_dir, 'r').readlines()\n",
    "train_inputs = tokenizer(codes, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "labels = tokenizer(docs, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "train_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "train_inputs[\"labels\"] = [\n",
    "    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in train_inputs[\"labels\"]\n",
    "]\n",
    "train_inputs[\"labels_attention_mask\"] = labels[\"attention_mask\"].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:36.651021Z",
     "start_time": "2024-05-29T17:23:26.720633Z"
    }
   },
   "id": "a90369e5c2cd7e9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = Dataset.from_dict(train_inputs)\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'labels_attention_mask'])\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.110877Z",
     "start_time": "2024-05-29T17:23:36.652383Z"
    }
   },
   "id": "d16ecead06b295e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'labels_attention_mask'],\n",
       "    num_rows: 55538\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_data, batch_size=15, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.114750Z",
     "start_time": "2024-05-29T17:23:43.112091Z"
    }
   },
   "id": "bbbc25db4e205ee4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = 5e-5\n",
    "warmup_steps = 500\n",
    "num_epochs = 10\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.118314Z",
     "start_time": "2024-05-29T17:23:43.115796Z"
    }
   },
   "id": "93def6bddc5185f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_bleu(model, device, src_dir, tgt_dir, tokenizer):\n",
    "    model.eval()\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    all_summaries = []\n",
    "    batch_size = 32\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        source = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = source[\"input_ids\"].to(device)\n",
    "        attention_mask = source[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=50)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[summary.rstrip().lower()[:-1]+' .'] for summary in all_summaries]))\n",
    "    references = dict(enumerate([[target.rstrip().lower()] for target in targets]))\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    return bleu\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.125715Z",
     "start_time": "2024-05-29T17:23:43.120756Z"
    }
   },
   "id": "20c792feb8e120d9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# measure model performance before fine-tuning\n",
    "# eval_bleu(model, device, test_source_dir, test_target_dir, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.128835Z",
     "start_time": "2024-05-29T17:23:43.126657Z"
    }
   },
   "id": "85697ccd52021c55",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.133604Z",
     "start_time": "2024-05-29T17:23:43.129683Z"
    }
   },
   "id": "eec0230d883b879e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap the train_loader with tqdm for a progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Load batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            labels_attention_mask = batch['labels_attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=labels,\n",
    "                            decoder_attention_mask=labels_attention_mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_epoch_loss = total_loss / len(train_loader)\n",
    "        # evaluate model performance every 5 epochs\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            bleu = eval_bleu(model, device, validation_source_dir, validation_target_dir, tokenizer)\n",
    "            print(\"validation BLEU: \", bleu)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_epoch_loss}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.139679Z",
     "start_time": "2024-05-29T17:23:43.134451Z"
    }
   },
   "id": "39822520cbb23be2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "# train(model, device, train_loader, optimizer, scheduler, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.142709Z",
     "start_time": "2024-05-29T17:23:43.140660Z"
    }
   },
   "id": "fde269282f97a59b",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# measure model performance after fine-tuning\n",
    "# eval_bleu(model, device, test_source_dir, test_target_dir, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.145773Z",
     "start_time": "2024-05-29T17:23:43.143750Z"
    }
   },
   "id": "b69e42173100fce5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "# model.save_pretrained(\"codet5p_ft_epoch_{}_lang_{}\".format(num_epochs, lang))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.148706Z",
     "start_time": "2024-05-29T17:23:43.146604Z"
    }
   },
   "id": "b1eff1055b378dfd",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.152766Z",
     "start_time": "2024-05-29T17:23:43.149527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(TorchDataset):\n",
    "    def __init__(self, hypotheses, references):\n",
    "        self.hypotheses = list(chain.from_iterable(hypotheses.values()))\n",
    "        self.references = list(chain.from_iterable(references.values()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hypotheses)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.hypotheses[idx], self.references[idx]"
   ],
   "id": "96f10b45e77cd81a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_with_my_beam_search(model,\n",
    "                             device,\n",
    "                             src_dir,\n",
    "                             tgt_dir,\n",
    "                             tokenizer,\n",
    "                             beam_search,\n",
    "                             batch_size=16,\n",
    "                             beam_size=4,\n",
    "                             num_return_sequences=4,\n",
    "                             temperature=1.0):\n",
    "    model.eval()\n",
    "    bertscore = load(\"bertscore\")\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    separated_hypotheses = []\n",
    "    all_summaries = []\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        source = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=350)\n",
    "        input_ids = source[\"input_ids\"].to(device)\n",
    "        attention_mask = source[\"attention_mask\"].to(device)\n",
    "        summaries = beam_search(model,\n",
    "                                input_ids,\n",
    "                                attention_mask,\n",
    "                                tokenizer,\n",
    "                                beam_size,\n",
    "                                num_return_sequences,\n",
    "                                temperature=temperature)\n",
    "        separated_hypotheses.append([re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .') for summary in summaries])\n",
    "        all_summaries.extend(summaries)\n",
    "    all_distinct_unigrams_ratio = [distinct_n_corpus_level(preds, n=1) for preds in separated_hypotheses]\n",
    "    all_distinct_bigrams_ratio = [distinct_n_corpus_level(preds, n=2) for preds in separated_hypotheses]\n",
    "    all_self_bleu = [self_bleu_score(preds) for preds in separated_hypotheses]\n",
    "    all_self_bert_precision = []\n",
    "    all_self_bert_recall = []\n",
    "    all_self_bert_f1 = []\n",
    "    for preds in separated_hypotheses:\n",
    "        bert_precision, bert_recall, bert_f1 = self_bert_score(preds, bertscore)\n",
    "        all_self_bert_precision.append(bert_precision)\n",
    "        all_self_bert_recall.append(bert_recall)\n",
    "        all_self_bert_f1.append(bert_f1)\n",
    "    average_distinct_unigrams_ratio = np.mean(all_distinct_unigrams_ratio)\n",
    "    average_distinct_bigrams_ratio = np.mean(all_distinct_bigrams_ratio)\n",
    "    average_self_bleu = np.mean(all_self_bleu)\n",
    "    average_self_bert_precision = np.mean(all_self_bert_precision)\n",
    "    average_self_bert_recall = np.mean(all_self_bert_recall)\n",
    "    average_self_bert_f1 = np.mean(all_self_bert_f1)\n",
    "    hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in all_summaries]))\n",
    "    # repeat targets for each generated sequence\n",
    "    repeated_targets = []\n",
    "    for target in targets:\n",
    "        repeated_targets.extend([target]*num_return_sequences)\n",
    "    references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "    #calculate oracle scores\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    reshaped_bleu = np.array(list(ind_bleu.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_bleu = np.max(reshaped_bleu, axis=1)\n",
    "    rouge_calculator = Rouge()\n",
    "    rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_rouge = np.array(list(ind_rouge.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_rouge = np.max(reshaped_rouge, axis=1)\n",
    "    meteor_calculator = Meteor()\n",
    "    meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_meteor = np.array(list(ind_meteor)).reshape(-1, num_return_sequences)\n",
    "    oracle_meteor = np.max(reshaped_meteor, axis=1)\n",
    "    print(\"Oracle scores, bleu: \", np.mean(oracle_bleu) * 100, \" rouge-l: \", np.mean(oracle_rouge) * 100, \" meteor: \", np.mean(oracle_meteor) * 100)\n",
    "\n",
    "    bert_precision = []\n",
    "    bert_recall = []\n",
    "    bert_f1 = []\n",
    "    # Define your batch size\n",
    "    batch_size = 5120\n",
    "    # Create an instance of your dataset\n",
    "    dataset = TextDataset(hypotheses, references)\n",
    "    # Create a DataLoader instance\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Iterating over batches and compute BERTScore for each with a progress bar\n",
    "    for hypotheses_batch, references_batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        batch_results = bertscore.compute(predictions=hypotheses_batch,\n",
    "                                          references=references_batch,\n",
    "                                          lang=\"en\")\n",
    "        bert_precision.extend(list(batch_results['precision']))\n",
    "        bert_recall.extend(list(batch_results['recall']))\n",
    "        bert_f1.extend(list(batch_results['f1']))\n",
    "    \n",
    "    bert_precision_array = np.array(bert_precision).reshape(-1, num_return_sequences)\n",
    "    best_bert_precision = np.max(bert_precision_array, axis=1)\n",
    "    \n",
    "    bert_recall_array = np.array(bert_recall).reshape(-1, num_return_sequences)\n",
    "    best_bert_recall = np.max(bert_recall_array, axis=1)\n",
    "    \n",
    "    bert_f1_array = np.array(bert_f1).reshape(-1, num_return_sequences)\n",
    "    best_bert_f1 = np.max(bert_f1_array, axis=1)\n",
    "    \n",
    "    # write results\n",
    "    # create the file if it does not exist\n",
    "    hyp_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"stochastic_beam_search_results_beam_{num_return_sequences}_temperature_{temperature}_hypotheses.json\"), 'w')\n",
    "    json.dump(hypotheses, hyp_fw, indent=4)\n",
    "    hyp_fw.close()\n",
    "    \n",
    "    ref_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"stochastic_beam_search_results_beam_{num_return_sequences}_temperature_{temperature}_references.json\"), 'w')\n",
    "    json.dump(references, ref_fw, indent=4)\n",
    "    ref_fw.close()\n",
    "    \n",
    "    file_name = \"stochastic_beam_search_results.txt\"\n",
    "    with open(os.path.join(\"diverse_decoding_results\", file_name), \"a\") as f:\n",
    "        f.write(f\"temperature: {temperature}, num_return_sequences: {num_return_sequences}, distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f}, distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f}, self_bleu: {average_self_bleu * 100: .4f}, self_bert_precision: {average_self_bert_precision * 100: .4f}, self_bert_recall: {average_self_bert_recall * 100: .4f}, self_bert_f1: {average_self_bert_f1 * 100: .4f}, average_bleu: {np.mean(list(ind_bleu.values())) * 100: .4f}, average_rouge: {np.mean(list(ind_rouge.values())) * 100: .4f}, average_meteor: {np.mean(list(ind_meteor)) * 100: .4f}, oracle_bleu: {np.mean(oracle_bleu) * 100: .4f}, oracle_rouge: {np.mean(oracle_rouge) * 100: .4f}, oracle_meteor: {np.mean(oracle_meteor) * 100: .4f}, oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f}, oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f}, oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n",
    "    \n",
    "    return hypotheses, references"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.181168Z",
     "start_time": "2024-05-29T17:23:43.154060Z"
    }
   },
   "id": "614c6bacc892034b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "def my_beam_search(model,\n",
    "                   encoder_input_ids,\n",
    "                   encoder_attention_mask,\n",
    "                   tokenizer,\n",
    "                   num_beams=4,\n",
    "                   num_return_seq=4,\n",
    "                   max_length=70):\n",
    "    model.eval()\n",
    "    beam_scorer = BeamSearchScorer(batch_size=encoder_input_ids.size(0),\n",
    "                                   num_beams=num_beams,\n",
    "                                   device=encoder_input_ids.device,\n",
    "                                   num_beam_hyps_to_keep=num_return_seq,\n",
    "                                   max_length=max_length,)\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    decoder_input_ids = torch.full((encoder_input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=encoder_input_ids.device)\n",
    "\n",
    "    expanded_encoder_input_ids = encoder_input_ids.repeat_interleave(num_beams, dim=0)\n",
    "    expanded_encoder_attention_mask = encoder_attention_mask.repeat_interleave(num_beams, dim=0)\n",
    "    expanded_decoder_input_ids = decoder_input_ids.repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "    batch_size = encoder_input_ids.size(0)\n",
    "    batch_beam_size, cur_len = expanded_decoder_input_ids.shape\n",
    "\n",
    "    assert num_beams * batch_size == batch_beam_size\n",
    "\n",
    "    scores = ()\n",
    "    raw_logits = ()\n",
    "    beam_indices = (tuple(() for _ in range(batch_beam_size)))\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=expanded_decoder_input_ids.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "\n",
    "    decoder_prompt_len = expanded_decoder_input_ids.shape[-1]\n",
    "    encoder_output = model.get_encoder()(expanded_encoder_input_ids, expanded_encoder_attention_mask)\n",
    "\n",
    "    while cur_len < max_length:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoder_outputs=encoder_output,\n",
    "                            decoder_input_ids=expanded_decoder_input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_scores = nn.functional.log_softmax(\n",
    "            next_token_logits, dim=-1\n",
    "        )  # (batch_size * num_beams, vocab_size)\n",
    "        next_token_scores_processed = logits_processor(expanded_decoder_input_ids, next_token_scores)\n",
    "        next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
    "            next_token_scores_processed\n",
    "        )\n",
    "        vocab_size = next_token_scores.shape[-1]\n",
    "        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "        # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n",
    "        n_eos_tokens = len(eos_token_id) if eos_token_id else 0\n",
    "        next_token_scores, next_tokens = torch.topk(\n",
    "            next_token_scores, max(2, 1 + n_eos_tokens) * num_beams, dim=1, largest=True, sorted=True\n",
    "        )\n",
    "        next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "        next_tokens = next_tokens % vocab_size\n",
    "        scores += (next_token_scores_processed,)\n",
    "        raw_logits += (next_token_logits,)\n",
    "        # stateless\n",
    "        beam_outputs = beam_scorer.process(\n",
    "            expanded_decoder_input_ids,\n",
    "            next_token_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            beam_indices=beam_indices,\n",
    "            decoder_prompt_len=decoder_prompt_len,\n",
    "        )\n",
    "\n",
    "        beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "        beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "        beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "        expanded_decoder_input_ids = torch.cat([expanded_decoder_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "        cur_len = cur_len + 1\n",
    "        if beam_scorer.is_done:\n",
    "            break\n",
    "    sequence_outputs = beam_scorer.finalize(\n",
    "        expanded_decoder_input_ids,\n",
    "        beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        max_length=max_length,\n",
    "        beam_indices=beam_indices,\n",
    "        decoder_prompt_len=decoder_prompt_len,\n",
    "    )\n",
    "    decoded = tokenizer.batch_decode(sequence_outputs[\"sequences\"], skip_special_tokens=True)\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.191481Z",
     "start_time": "2024-05-29T17:23:43.182206Z"
    }
   },
   "id": "847d804bb2c49544",
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "def stochastic_beam_search(model,\n",
    "                           encoder_input_ids,\n",
    "                           encoder_attention_mask,\n",
    "                           tokenizer,\n",
    "                           num_beams=4,\n",
    "                           num_return_seq=4,\n",
    "                           max_length=60,\n",
    "                           temperature=1.,):\n",
    "    model.eval()\n",
    "    beam_scorer = BeamSearchScorer(batch_size=encoder_input_ids.size(0),\n",
    "                                   num_beams=num_beams,\n",
    "                                   device=encoder_input_ids.device,\n",
    "                                   num_beam_hyps_to_keep=num_return_seq,\n",
    "                                   max_length=max_length,)\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    eos_token_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.pad_token_id\n",
    "    if isinstance(eos_token_id, int):\n",
    "        eos_token_id = [eos_token_id]\n",
    "    decoder_input_ids = torch.full((encoder_input_ids.size(0), 1), pad_token_id, dtype=torch.long, device=encoder_input_ids.device)\n",
    "\n",
    "    expanded_encoder_input_ids = encoder_input_ids.repeat_interleave(num_beams, dim=0)\n",
    "    expanded_encoder_attention_mask = encoder_attention_mask.repeat_interleave(num_beams, dim=0)\n",
    "    expanded_decoder_input_ids = decoder_input_ids.repeat_interleave(num_beams, dim=0)\n",
    "\n",
    "    batch_size = encoder_input_ids.size(0)\n",
    "    batch_beam_size, cur_len = expanded_decoder_input_ids.shape\n",
    "\n",
    "    assert num_beams * batch_size == batch_beam_size\n",
    "\n",
    "    scores = ()\n",
    "    raw_logits = ()\n",
    "    beam_indices = (tuple(() for _ in range(batch_beam_size)))\n",
    "\n",
    "    beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=expanded_decoder_input_ids.device)\n",
    "    beam_scores[:, 1:] = -1e9\n",
    "    beam_scores = beam_scores.view((batch_size * num_beams,))\n",
    "    gumbel_scores = torch.zeros((batch_size * num_beams, max_length), dtype=torch.float, device=expanded_decoder_input_ids.device)\n",
    "\n",
    "    decoder_prompt_len = expanded_decoder_input_ids.shape[-1]\n",
    "    encoder_output = model.get_encoder()(expanded_encoder_input_ids, expanded_encoder_attention_mask)\n",
    "\n",
    "    while cur_len < max_length:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoder_outputs=encoder_output,\n",
    "                            decoder_input_ids=expanded_decoder_input_ids)\n",
    "        next_token_logits = outputs.logits[:, -1, :]\n",
    "        next_token_scores = nn.functional.log_softmax(\n",
    "            next_token_logits / temperature, dim=-1\n",
    "        )  # (batch_size * num_beams, vocab_size)\n",
    "        next_token_scores_processed = logits_processor(expanded_decoder_input_ids, next_token_scores)\n",
    "        next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n",
    "            next_token_scores_processed\n",
    "        )\n",
    "        if cur_len == 1:\n",
    "            cand_scores = sbsutils.gumbel_like(next_token_scores) + next_token_scores\n",
    "        else:\n",
    "            cand_scores, _ = sbsutils.gumbel_with_maximum(next_token_scores.view(batch_size, num_beams, -1), gumbel_scores.view(batch_size, num_beams, -1)[:, :, cur_len-1], -1)\n",
    "            cand_scores = cand_scores.view(batch_size * num_beams, -1)\n",
    "        next_token_scores = cand_scores\n",
    "        vocab_size = next_token_scores.shape[-1]\n",
    "        next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n",
    "        # Sample 1 + len(eos_token_id) next tokens for each beam so we have at least 1 non eos token per beam.\n",
    "        n_eos_tokens = len(eos_token_id) if eos_token_id else 0\n",
    "        next_token_scores, next_tokens = torch.topk(\n",
    "            next_token_scores, max(2, 1 + n_eos_tokens) * num_beams, dim=1, largest=True, sorted=True\n",
    "        )\n",
    "        next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n",
    "        next_tokens = next_tokens % vocab_size\n",
    "        scores += (next_token_scores_processed,)\n",
    "        raw_logits += (next_token_logits,)\n",
    "        # stateless\n",
    "        beam_outputs = beam_scorer.process(\n",
    "            expanded_decoder_input_ids,\n",
    "            next_token_scores,\n",
    "            next_tokens,\n",
    "            next_indices,\n",
    "            pad_token_id=pad_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            beam_indices=beam_indices,\n",
    "            decoder_prompt_len=decoder_prompt_len,\n",
    "        )\n",
    "\n",
    "        beam_scores = beam_outputs[\"next_beam_scores\"]\n",
    "        beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n",
    "        beam_idx = beam_outputs[\"next_beam_indices\"]\n",
    "        gumbel_scores[:, cur_len] = beam_scores\n",
    "        expanded_decoder_input_ids = torch.cat([expanded_decoder_input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n",
    "        cur_len = cur_len + 1\n",
    "        if beam_scorer.is_done:\n",
    "            break\n",
    "    sequence_outputs = beam_scorer.finalize(\n",
    "        expanded_decoder_input_ids,\n",
    "        beam_scores,\n",
    "        next_tokens,\n",
    "        next_indices,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        max_length=max_length,\n",
    "        beam_indices=beam_indices,\n",
    "        decoder_prompt_len=decoder_prompt_len,\n",
    "    )\n",
    "    decoded = [tokenizer.decode(output, skip_special_tokens=True) for output in sequence_outputs[\"sequences\"]]\n",
    "    return decoded"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.202905Z",
     "start_time": "2024-05-29T17:23:43.192426Z"
    }
   },
   "id": "bf03deda1b6974d5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# num_return_sequences_list = [10, 20]\n",
    "# temperature_list = [0.3]\n",
    "# for temperature in temperature_list:\n",
    "#     for num_return_sequences in num_return_sequences_list:\n",
    "#         print(\"temperature: \", temperature, \" num_return_sequences: \", num_return_sequences)\n",
    "#         hypotheses, references = eval_with_my_beam_search(model,\n",
    "#                                                           device,\n",
    "#                                                           test_source_dir,\n",
    "#                                                           test_target_dir,\n",
    "#                                                           tokenizer,\n",
    "#                                                           stochastic_beam_search,\n",
    "#                                                           batch_size=1,\n",
    "#                                                           beam_size=num_return_sequences,\n",
    "#                                                           num_return_sequences=num_return_sequences,\n",
    "#                                                           temperature=temperature)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.205884Z",
     "start_time": "2024-05-29T17:23:43.203646Z"
    }
   },
   "id": "2251120cff8123b8",
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "def distinct_with_beam_search(model,\n",
    "                              device,\n",
    "                              src_dir,\n",
    "                              tgt_dir,\n",
    "                              tokenizer,\n",
    "                              batch_size=16,\n",
    "                              beam_size=10,\n",
    "                              num_return_sequences=8):\n",
    "    model.eval()\n",
    "    bertscore = load(\"bertscore\")\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    separated_hypotheses = []\n",
    "    all_summaries = []\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        source = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = source[\"input_ids\"].to(device)\n",
    "        attention_mask = source[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids, \n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=100, \n",
    "                                       num_beams=beam_size, \n",
    "                                       num_return_sequences=num_return_sequences)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        separated_hypotheses.append([re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .') for summary in summaries])\n",
    "        all_summaries.extend(summaries)\n",
    "    all_distinct_unigrams_ratio = [distinct_n_corpus_level(preds, n=1) for preds in separated_hypotheses]\n",
    "    all_distinct_bigrams_ratio = [distinct_n_corpus_level(preds, n=2) for preds in separated_hypotheses]\n",
    "    all_self_bleu = [self_bleu_score(preds) for preds in separated_hypotheses]\n",
    "    all_self_bert_precision = []\n",
    "    all_self_bert_recall = []\n",
    "    all_self_bert_f1 = []\n",
    "    for preds in separated_hypotheses:\n",
    "        bert_precision, bert_recall, bert_f1 = self_bert_score(preds, bertscore)\n",
    "        all_self_bert_precision.append(bert_precision)\n",
    "        all_self_bert_recall.append(bert_recall)\n",
    "        all_self_bert_f1.append(bert_f1)\n",
    "    average_distinct_unigrams_ratio = np.mean(all_distinct_unigrams_ratio)\n",
    "    average_distinct_bigrams_ratio = np.mean(all_distinct_bigrams_ratio)\n",
    "    average_self_bleu = np.mean(all_self_bleu)\n",
    "    average_self_bert_precision = np.mean(all_self_bert_precision)\n",
    "    average_self_bert_recall = np.mean(all_self_bert_recall)\n",
    "    average_self_bert_f1 = np.mean(all_self_bert_f1)\n",
    "    hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in all_summaries]))\n",
    "    # repeat targets for each generated sequence\n",
    "    repeated_targets = []\n",
    "    for target in targets:\n",
    "        repeated_targets.extend([target]*num_return_sequences)\n",
    "    references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "    #calculate oracle scores\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    reshaped_bleu = np.array(list(ind_bleu.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_bleu = np.max(reshaped_bleu, axis=1)\n",
    "    rouge_calculator = Rouge()\n",
    "    rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_rouge = np.array(list(ind_rouge.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_rouge = np.max(reshaped_rouge, axis=1)\n",
    "    meteor_calculator = Meteor()\n",
    "    meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_meteor = np.array(list(ind_meteor)).reshape(-1, num_return_sequences)\n",
    "    oracle_meteor = np.max(reshaped_meteor, axis=1)\n",
    "    print(\"Oracle scores, bleu: \", np.mean(oracle_bleu) * 100, \" rouge-l: \", np.mean(oracle_rouge) * 100, \" meteor: \", np.mean(oracle_meteor) * 100)\n",
    "\n",
    "    bert_precision = []\n",
    "    bert_recall = []\n",
    "    bert_f1 = []\n",
    "    # Define your batch size\n",
    "    batch_size = 5120\n",
    "    # Create an instance of your dataset\n",
    "    dataset = TextDataset(hypotheses, references)\n",
    "    # Create a DataLoader instance\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Iterating over batches and compute BERTScore for each with a progress bar\n",
    "    for hypotheses_batch, references_batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        batch_results = bertscore.compute(predictions=hypotheses_batch,\n",
    "                                          references=references_batch,\n",
    "                                          lang=\"en\")\n",
    "        bert_precision.extend(list(batch_results['precision']))\n",
    "        bert_recall.extend(list(batch_results['recall']))\n",
    "        bert_f1.extend(list(batch_results['f1']))\n",
    "\n",
    "    bert_precision_array = np.array(bert_precision).reshape(-1, num_return_sequences)\n",
    "    best_bert_precision = np.max(bert_precision_array, axis=1)\n",
    "\n",
    "    bert_recall_array = np.array(bert_recall).reshape(-1, num_return_sequences)\n",
    "    best_bert_recall = np.max(bert_recall_array, axis=1)\n",
    "\n",
    "    bert_f1_array = np.array(bert_f1).reshape(-1, num_return_sequences)\n",
    "    best_bert_f1 = np.max(bert_f1_array, axis=1)\n",
    "\n",
    "    hyp_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"beam_search_results_beam_{num_return_sequences}_hypotheses.json\"), 'w')\n",
    "    json.dump(hypotheses, hyp_fw, indent=4)\n",
    "    hyp_fw.close()\n",
    "\n",
    "    ref_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"beam_search_results_beam_{num_return_sequences}_references.json\"), 'w')\n",
    "    json.dump(references, ref_fw, indent=4)\n",
    "    ref_fw.close()\n",
    "    \n",
    "    file_name = \"beam_search_results.txt\"\n",
    "    with open(os.path.join(\"diverse_decoding_results\", file_name), \"a\") as f:\n",
    "        f.write(f\"num_return_sequences: {num_return_sequences}, distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f}, distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f}, self_bleu: {average_self_bleu * 100: .4f}, self_bert_precision: {average_self_bert_precision * 100: .4f}, self_bert_recall: {average_self_bert_recall * 100: .4f}, self_bert_f1: {average_self_bert_f1 * 100: .4f}, average_bleu: {np.mean(list(ind_bleu.values())) * 100: .4f}, average_rouge: {np.mean(list(ind_rouge.values())) * 100: .4f}, average_meteor: {np.mean(list(ind_meteor)) * 100: .4f}, oracle_bleu: {np.mean(oracle_bleu) * 100: .4f}, oracle_rouge: {np.mean(oracle_rouge) * 100: .4f}, oracle_meteor: {np.mean(oracle_meteor) * 100: .4f}, oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f}, oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f}, oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n",
    "                \n",
    "    return hypotheses, references"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:23:43.220788Z",
     "start_time": "2024-05-29T17:23:43.206739Z"
    }
   },
   "id": "47c3e61c2fd3e6b0",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "num_return_sequences_list = [10, 20]\n",
    "for num_return_sequences in num_return_sequences_list:\n",
    "    hypotheses, references = distinct_with_beam_search(model,\n",
    "                                                       device,\n",
    "                                                       test_source_dir,\n",
    "                                                       test_target_dir,\n",
    "                                                       tokenizer,\n",
    "                                                       batch_size=4,\n",
    "                                                       beam_size=num_return_sequences,\n",
    "                                                       num_return_sequences=num_return_sequences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-29T17:23:43.221632Z"
    }
   },
   "id": "ed74cdbdc45bd27e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4626/4626 [26:50<00:00,  2.87it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle scores, bleu:  45.000365738693944  rouge-l:  61.78330220564606  meteor:  42.34021246836098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 37/37 [02:20<00:00,  3.79s/it]\n",
      "100%|██████████| 4626/4626 [42:25<00:00,  1.82it/s] \n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def distinct_with_sampling(model,\n",
    "                           device,\n",
    "                           src_dir,\n",
    "                           tgt_dir,\n",
    "                           tokenizer,\n",
    "                           batch_size=1,\n",
    "                           num_sample=100,\n",
    "                           temperature=1.0,\n",
    "                           top_k=50,\n",
    "                           top_p=1,\n",
    "                           num_distinct_summary_list=(4, 8, 12, 16, 20)):\n",
    "    model.eval()\n",
    "    bertscore = load(\"bertscore\")\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    all_summaries = []\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        source = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = source[\"input_ids\"].to(device)\n",
    "        attention_mask = source[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=80,\n",
    "                                       do_sample=True,\n",
    "                                       # top_k=top_k,\n",
    "                                       # top_p=top_p,\n",
    "                                       temperature=temperature,\n",
    "                                       num_return_sequences=num_sample)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_summaries.extend(summaries)\n",
    "\n",
    "    # separate the summaries into nested list of num_latents\n",
    "    separated_summaries = [all_summaries[i:i+num_sample] for i in range(0, len(all_summaries), num_sample)]\n",
    "    predictions = []\n",
    "    for num_distinct_summary in num_distinct_summary_list:\n",
    "        # keep at most num_distinct_summary distinct summaries\n",
    "        distinct_summaries = []\n",
    "        distinct_guesses = []\n",
    "        repeated_targets = []\n",
    "        separated_hypotheses = []\n",
    "        for i, summaries in enumerate(separated_summaries):\n",
    "            distinct_sum = list(set(summaries))\n",
    "            if len(distinct_sum) > num_distinct_summary:\n",
    "                distinct_sum = distinct_sum[:num_distinct_summary]\n",
    "            distinct_guesses.append(len(distinct_sum))\n",
    "            separated_hypotheses.append([re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .') for summary in distinct_sum])\n",
    "            distinct_summaries.extend(distinct_sum)\n",
    "            repeated_targets.extend([targets[i]]*len(distinct_sum))\n",
    "        predictions.append(separated_hypotheses)\n",
    "        all_distinct_unigrams_ratio = [distinct_n_corpus_level(preds, n=1) for preds in separated_hypotheses]\n",
    "        all_distinct_bigrams_ratio = [distinct_n_corpus_level(preds, n=2) for preds in separated_hypotheses]\n",
    "        all_self_bleu = [self_bleu_score(preds) for preds in separated_hypotheses]\n",
    "        all_self_bert_precision = []\n",
    "        all_self_bert_recall = []\n",
    "        all_self_bert_f1 = []\n",
    "        for preds in separated_hypotheses:\n",
    "            bert_precision, bert_recall, bert_f1 = self_bert_score(preds, bertscore)\n",
    "            all_self_bert_precision.append(bert_precision)\n",
    "            all_self_bert_recall.append(bert_recall)\n",
    "            all_self_bert_f1.append(bert_f1)\n",
    "        average_distinct_unigrams_ratio = np.mean(all_distinct_unigrams_ratio)\n",
    "        average_distinct_bigrams_ratio = np.mean(all_distinct_bigrams_ratio)\n",
    "        average_self_bleu = np.mean(all_self_bleu)\n",
    "        average_self_bert_precision = np.mean(all_self_bert_precision)\n",
    "        average_self_bert_recall = np.mean(all_self_bert_recall)\n",
    "        average_self_bert_f1 = np.mean(all_self_bert_f1)\n",
    "        hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in distinct_summaries]))\n",
    "        references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "        #calculate oracle scores\n",
    "        print(\"average distinct guesses: \", np.mean(distinct_guesses))\n",
    "        _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "        ind_bleu_input = iter(list(ind_bleu.values()))\n",
    "        sliced_bleu = [list(islice(ind_bleu_input, elem))\n",
    "                       for elem in distinct_guesses]\n",
    "        np_sliced_bleu = np.array(list(zip_longest(*sliced_bleu, fillvalue=0))).T\n",
    "        oracle_bleu = np.max(np_sliced_bleu, axis=1)\n",
    "\n",
    "        rouge_calculator = Rouge()\n",
    "        rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "        ind_rouge_input = iter(list(ind_rouge.values()))\n",
    "        sliced_rouge = [list(islice(ind_rouge_input, elem))\n",
    "                        for elem in distinct_guesses]\n",
    "        np_sliced_rouge = np.array(list(zip_longest(*sliced_rouge, fillvalue=0))).T\n",
    "        oracle_rouge = np.max(np_sliced_rouge, axis=1)\n",
    "\n",
    "        meteor_calculator = Meteor()\n",
    "        meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "        ind_meteor_input = iter(list(ind_meteor))\n",
    "        sliced_meteor = [list(islice(ind_meteor_input, elem))\n",
    "                         for elem in distinct_guesses]\n",
    "        np_sliced_meteor = np.array(list(zip_longest(*sliced_meteor, fillvalue=0))).T\n",
    "        oracle_meteor = np.max(np_sliced_meteor, axis=1)\n",
    "        print(\"Oracle scores, bleu: \", np.mean(oracle_bleu) * 100, \" rouge-l: \", np.mean(oracle_rouge) * 100, \" meteor: \", np.mean(oracle_meteor) * 100)\n",
    "\n",
    "\n",
    "        bert_precision = []\n",
    "        bert_recall = []\n",
    "        bert_f1 = []\n",
    "        # Define your batch size\n",
    "        batch_size = 5120\n",
    "        # Create an instance of your dataset\n",
    "        dataset = TextDataset(hypotheses, references)\n",
    "        # Create a DataLoader instance\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        # Iterating over batches and compute BERTScore for each with a progress bar\n",
    "        for hypotheses_batch, references_batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "            batch_results = bertscore.compute(predictions=hypotheses_batch,\n",
    "                                              references=references_batch,\n",
    "                                              lang=\"en\")\n",
    "            bert_precision.extend(list(batch_results['precision']))\n",
    "            bert_recall.extend(list(batch_results['recall']))\n",
    "            bert_f1.extend(list(batch_results['f1']))\n",
    "\n",
    "        bert_precision_input = iter(bert_precision)\n",
    "        sliced_bert_precision = [list(islice(bert_precision_input, elem))\n",
    "                                 for elem in distinct_guesses]\n",
    "        np_sliced_bert_precision = np.array(list(zip_longest(*sliced_bert_precision, fillvalue=0))).T\n",
    "        best_bert_precision = np.max(np_sliced_bert_precision, axis=1)\n",
    "\n",
    "        bert_recall_input = iter(bert_recall)\n",
    "        sliced_bert_recall = [list(islice(bert_recall_input, elem))\n",
    "                              for elem in distinct_guesses]\n",
    "        np_sliced_bert_recall = np.array(list(zip_longest(*sliced_bert_recall, fillvalue=0))).T\n",
    "        best_bert_recall = np.max(np_sliced_bert_recall, axis=1)\n",
    "\n",
    "        bert_f1_input = iter(bert_f1)\n",
    "        sliced_bert_f1 = [list(islice(bert_f1_input, elem))\n",
    "                          for elem in distinct_guesses]\n",
    "        np_sliced_bert_f1 = np.array(list(zip_longest(*sliced_bert_f1, fillvalue=0))).T\n",
    "        best_bert_f1 = np.max(np_sliced_bert_f1, axis=1)\n",
    "        \n",
    "        hyp_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"sampling_results_temperature_{temperature}_num_distinct_summary_{num_distinct_summary}_hypotheses.json\"), 'w')\n",
    "        json.dump(hypotheses, hyp_fw, indent=4)\n",
    "        hyp_fw.close()\n",
    "        \n",
    "        ref_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"sampling_results_temperature_{temperature}_num_distinct_summary_{num_distinct_summary}_references.json\"), 'w')\n",
    "        json.dump(references, ref_fw, indent=4)\n",
    "        ref_fw.close()\n",
    "        \n",
    "        file_name = \"sampling_results.txt\"\n",
    "        with open(os.path.join(\"diverse_decoding_results\", file_name), \"a\") as f:\n",
    "            f.write(f\"temperature: {temperature}, top_p: {top_p}, top_k: {top_k}, num_distinct_summary: {num_distinct_summary}, distinct_guesses: {np.mean(distinct_guesses)}, distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f}, distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f}, self_bleu: {average_self_bleu * 100: .4f}, self_bert_precision: {average_self_bert_precision * 100: .4f}, self_bert_recall: {average_self_bert_recall * 100: .4f}, self_bert_f1: {average_self_bert_f1 * 100: .4f}, average_bleu: {np.mean(list(ind_bleu.values())) * 100: .4f}, average_rouge: {np.mean(list(ind_rouge.values())) * 100: .4f}, average_meteor: {np.mean(list(ind_meteor)) * 100: .4f}, oracle_bleu: {np.mean(oracle_bleu) * 100: .4f}, oracle_rouge: {np.mean(oracle_rouge) * 100: .4f}, oracle_meteor: {np.mean(oracle_meteor) * 100: .4f}, oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f}, oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f}, oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n",
    "            \n",
    "    return predictions\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36d9d3900a44e943",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for temperature in [1.0]:\n",
    "    hypotheses = distinct_with_sampling(model,\n",
    "                                        device,\n",
    "                                        test_source_dir,\n",
    "                                        test_target_dir,\n",
    "                                        tokenizer,\n",
    "                                        batch_size=1,\n",
    "                                        num_sample=100,\n",
    "                                        temperature=temperature,\n",
    "                                        # top_k=50,\n",
    "                                        # top_p=1,\n",
    "                                        num_distinct_summary_list=(10, 20))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "19fe3e5926ca2b01",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def distinct_with_diverse_beam_search(model,\n",
    "                                      device,\n",
    "                                      src_dir,\n",
    "                                      tgt_dir,\n",
    "                                      tokenizer,\n",
    "                                      batch_size=16,\n",
    "                                      beam_size=10,\n",
    "                                      num_return_sequences=8,\n",
    "                                      diversity_penalty=10.0):\n",
    "    model.eval()\n",
    "    bertscore = load(\"bertscore\")\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    separated_hypotheses = []\n",
    "    all_summaries = []\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        source = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = source[\"input_ids\"].to(device)\n",
    "        attention_mask = source[\"attention_mask\"].to(device)\n",
    "        generated_ids = model.generate(input_ids,\n",
    "                                       attention_mask=attention_mask,\n",
    "                                       max_length=100,\n",
    "                                       diversity_penalty=diversity_penalty,\n",
    "                                       num_beam_groups=2,\n",
    "                                       num_beams=beam_size,\n",
    "                                       num_return_sequences=num_return_sequences)\n",
    "        summaries = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        separated_hypotheses.append([re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .') for summary in summaries])\n",
    "        all_summaries.extend(summaries)\n",
    "    all_distinct_unigrams_ratio = [distinct_n_corpus_level(preds, n=1) for preds in separated_hypotheses]\n",
    "    all_distinct_bigrams_ratio = [distinct_n_corpus_level(preds, n=2) for preds in separated_hypotheses]\n",
    "    all_self_bleu = [self_bleu_score(preds) for preds in separated_hypotheses]\n",
    "    all_self_bert_precision = []\n",
    "    all_self_bert_recall = []\n",
    "    all_self_bert_f1 = []\n",
    "    for preds in separated_hypotheses:\n",
    "        bert_precision, bert_recall, bert_f1 = self_bert_score(preds, bertscore)\n",
    "        all_self_bert_precision.append(bert_precision)\n",
    "        all_self_bert_recall.append(bert_recall)\n",
    "        all_self_bert_f1.append(bert_f1)\n",
    "    average_distinct_unigrams_ratio = np.mean(all_distinct_unigrams_ratio)\n",
    "    average_distinct_bigrams_ratio = np.mean(all_distinct_bigrams_ratio)\n",
    "    average_self_bleu = np.mean(all_self_bleu)\n",
    "    average_self_bert_precision = np.mean(all_self_bert_precision)\n",
    "    average_self_bert_recall = np.mean(all_self_bert_recall)\n",
    "    average_self_bert_f1 = np.mean(all_self_bert_f1)\n",
    "    hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in all_summaries]))\n",
    "    # repeat targets for each generated sequence\n",
    "    repeated_targets = []\n",
    "    for target in targets:\n",
    "        repeated_targets.extend([target]*num_return_sequences)\n",
    "    references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "    #calculate oracle scores\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    reshaped_bleu = np.array(list(ind_bleu.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_bleu = np.max(reshaped_bleu, axis=1)\n",
    "    rouge_calculator = Rouge()\n",
    "    rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_rouge = np.array(list(ind_rouge.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_rouge = np.max(reshaped_rouge, axis=1)\n",
    "    meteor_calculator = Meteor()\n",
    "    meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_meteor = np.array(list(ind_meteor)).reshape(-1, num_return_sequences)\n",
    "    oracle_meteor = np.max(reshaped_meteor, axis=1)\n",
    "    print(\"Oracle scores, bleu: \", np.mean(oracle_bleu) * 100, \" rouge-l: \", np.mean(oracle_rouge) * 100, \" meteor: \", np.mean(oracle_meteor) * 100)\n",
    "\n",
    "    \n",
    "    bert_precision = []\n",
    "    bert_recall = []\n",
    "    bert_f1 = []\n",
    "    # Define your batch size\n",
    "    batch_size = 5120\n",
    "    # Create an instance of your dataset\n",
    "    dataset = TextDataset(hypotheses, references)\n",
    "    # Create a DataLoader instance\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    # Iterating over batches and compute BERTScore for each with a progress bar\n",
    "    for hypotheses_batch, references_batch in tqdm(dataloader, desc=\"Processing batches\"):\n",
    "        batch_results = bertscore.compute(predictions=hypotheses_batch,\n",
    "                                          references=references_batch,\n",
    "                                          lang=\"en\")\n",
    "        bert_precision.extend(list(batch_results['precision']))\n",
    "        bert_recall.extend(list(batch_results['recall']))\n",
    "        bert_f1.extend(list(batch_results['f1']))\n",
    "\n",
    "    bert_precision_array = np.array(bert_precision).reshape(-1, num_return_sequences)\n",
    "    best_bert_precision = np.max(bert_precision_array, axis=1)\n",
    "\n",
    "    bert_recall_array = np.array(bert_recall).reshape(-1, num_return_sequences)\n",
    "    best_bert_recall = np.max(bert_recall_array, axis=1)\n",
    "\n",
    "    bert_f1_array = np.array(bert_f1).reshape(-1, num_return_sequences)\n",
    "    best_bert_f1 = np.max(bert_f1_array, axis=1)\n",
    "    \n",
    "    hyp_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"diverse_beam_search_results_beam_{num_return_sequences}_diversity_penalty_{diversity_penalty}_num_beam_groups_{2}_hypotheses.json\"), 'w')\n",
    "    json.dump(hypotheses, hyp_fw, indent=4)\n",
    "    hyp_fw.close()\n",
    "    \n",
    "    ref_fw = open(os.path.join(\"diverse_decoding_results\", \"generated\", lang, f\"diverse_beam_search_results_beam_{num_return_sequences}_diversity_penalty_{diversity_penalty}_num_beam_groups_{2}_references.json\"), 'w')\n",
    "    json.dump(references, ref_fw, indent=4)\n",
    "    ref_fw.close()\n",
    "    \n",
    "    file_name = \"diverse_beam_search_results.txt\"\n",
    "    \n",
    "    with open(os.path.join(\"diverse_decoding_results\", file_name), \"a\") as f:\n",
    "        f.write(f\"num_return_sequences: {num_return_sequences}, num_beam_groups: {2}, beam_size: {beam_size}, diversity_penalty: {diversity_penalty}, distinct_unigrams_ratio: {average_distinct_unigrams_ratio * 100: .4f}, distinct_bigrams_ratio: {average_distinct_bigrams_ratio * 100: .4f}, self_bleu: {average_self_bleu * 100: .4f}, self_bert_precision: {average_self_bert_precision * 100: .4f}, self_bert_recall: {average_self_bert_recall * 100: .4f}, self_bert_f1: {average_self_bert_f1 * 100: .4f}, average_bleu: {np.mean(list(ind_bleu.values())) * 100: .4f}, average_rouge: {np.mean(list(ind_rouge.values())) * 100: .4f}, average_meteor: {np.mean(list(ind_meteor)) * 100: .4f}, oracle_bleu: {np.mean(oracle_bleu) * 100: .4f}, oracle_rouge: {np.mean(oracle_rouge) * 100: .4f}, oracle_meteor: {np.mean(oracle_meteor) * 100: .4f}, oracle_bert_precision: {np.mean(best_bert_precision) * 100: .4f}, oracle_bert_recall: {np.mean(best_bert_recall) * 100: .4f}, oracle_bert_f1: {np.mean(best_bert_f1) * 100: .4f}\\n\")\n",
    "\n",
    "    return hypotheses, references"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a4e4137994b9ad25",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "diversity_penalty_list = [25.]\n",
    "num_return_sequences_list = [10, 20]\n",
    "for diversity_penalty in diversity_penalty_list:\n",
    "    for num_return_sequences in num_return_sequences_list:\n",
    "        print(\"diversity penalty: \", diversity_penalty, \" num_return_sequences: \", num_return_sequences)\n",
    "        hypotheses, references = distinct_with_diverse_beam_search(model,\n",
    "                                                                   device,\n",
    "                                                                   test_source_dir,\n",
    "                                                                   test_target_dir,\n",
    "                                                                   tokenizer,\n",
    "                                                                   batch_size=1,\n",
    "                                                                   beam_size=num_return_sequences,\n",
    "                                                                   num_return_sequences=num_return_sequences,\n",
    "                                                                   diversity_penalty=diversity_penalty)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "651502585af4cfa1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
