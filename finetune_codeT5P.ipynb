{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import torch.nn as nn\n",
    "from c2nl.eval.bleu import corpus_bleu\n",
    "from c2nl.eval.rouge import Rouge\n",
    "from c2nl.eval.meteor import Meteor\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, TrainingArguments, Trainer, get_linear_schedule_with_warmup, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4508f0b329b2a45",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value)  # Python random module\n",
    "    np.random.seed(seed_value)  # Numpy module\n",
    "    torch.manual_seed(seed_value)  # PyTorch\n",
    "    torch.cuda.manual_seed(seed_value)  # PyTorch CUDA\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch CUDA (for multi-GPU setups)\n",
    "    torch.backends.cudnn.deterministic = True  # For CUDA backend\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # For Python hash seeding\n",
    "# Example usage\n",
    "set_seed(42)  # Replace 42 with your desired seed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd2b93a6082948c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "lang = \"python\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if lang == 'java':\n",
    "    base_model ='Salesforce/codet5p-220m'\n",
    "else:\n",
    "    base_model = 'Salesforce/codet5p-220m-bimodal'\n",
    "if lang == 'java':\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m'\n",
    "else:\n",
    "    base_model_tokenizer = 'Salesforce/codet5p-220m-bimodal'\n",
    "if 'bimodal' in base_model or 'python' in base_model:\n",
    "    print(\"using auto model\")\n",
    "    model = AutoModel.from_pretrained(base_model, trust_remote_code=True).to(device)\n",
    "else:\n",
    "    print(\"using t5 conditional generation model\")\n",
    "    model = T5ForConditionalGeneration.from_pretrained(base_model, trust_remote_code=True).to(device)\n",
    "    \n",
    "checkpoint_dir = \"./codet5p_checkpoints\"\n",
    "checkpoint_name = f\"codet5p_ft_lang_{lang}_backbone\"\n",
    "max_input_length = 512\n",
    "max_target_length = 128"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fda6d29cc42617b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.cuda.get_device_name(device))",
   "id": "28bc8653b70fed8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_tokenizer)\n",
    "train_source_dir = \"./data/{}/train/code.original\".format(lang)\n",
    "train_target_dir = \"./data/{}/train/javadoc.original\".format(lang)\n",
    "validation_source_dir = \"./data/{}/dev/code.original\".format(lang)\n",
    "validation_target_dir = \"./data/{}/dev/javadoc.original\".format(lang)\n",
    "test_source_dir = \"./data/{}/test/code.original\".format(lang)\n",
    "test_target_dir = \"./data/{}/test/javadoc.original\".format(lang)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d09a30a9e05339e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# load my own data\n",
    "\n",
    "codes = open(train_source_dir, 'r').readlines()\n",
    "docs = open(train_target_dir, 'r').readlines()\n",
    "train_inputs = tokenizer(codes, max_length=max_input_length, padding=\"max_length\", truncation=True)\n",
    "labels = tokenizer(docs, max_length=max_target_length, padding=\"max_length\", truncation=True)\n",
    "train_inputs[\"labels\"] = labels[\"input_ids\"].copy()\n",
    "train_inputs[\"labels\"] = [\n",
    "    [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in train_inputs[\"labels\"]\n",
    "]\n",
    "train_inputs[\"labels_attention_mask\"] = labels[\"attention_mask\"].copy()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a90369e5c2cd7e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_data = Dataset.from_dict(train_inputs)\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'labels_attention_mask'])\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d16ecead06b295e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_data, batch_size=15, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbbc25db4e205ee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = 5e-5\n",
    "warmup_steps = 10000\n",
    "num_epochs = 200\n",
    "\n",
    "total_steps = len(train_loader) * num_epochs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93def6bddc5185f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def eval_bleu(model, device, src_dir, tgt_dir, tokenizer):\n",
    "    model.eval()\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    all_summaries = []\n",
    "    batch_size = 32\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        input_ids = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(device)\n",
    "        generated_ids = model.generate(input_ids, max_length=50)\n",
    "        summaries = [tokenizer.decode(generated_ids[j], skip_special_tokens=True) for j in range(len(batch))]\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[summary.rstrip().lower()[:-1]+' .'] for summary in all_summaries]))\n",
    "    references = dict(enumerate([[target.rstrip().lower()] for target in targets]))\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    return bleu\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20c792feb8e120d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eec0230d883b879e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "steps = 0\n",
    "curr_epoch = 0\n",
    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(checkpoint_name+\".pt\")]\n",
    "if len(checkpoints) > 0:\n",
    "    checkpoint = torch.load(os.path.join(checkpoint_dir, checkpoints[-1]))\n",
    "    model = model.from_pretrained(checkpoint_name).to(device)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    curr_epoch = checkpoint['epoch']\n",
    "    print(\"Loaded checkpoint: \", checkpoints[-1], \" at epoch \", curr_epoch)\n",
    "    print(f\"current loss: {checkpoint['loss']}\")\n",
    "else:\n",
    "    print(\"No checkpoints found\")"
   ],
   "id": "b67fb7df12be4be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# measure model performance before fine-tuning\n",
    "if curr_epoch < num_epochs:\n",
    "    print(\"Test BLEU: \", eval_bleu(model, device, test_source_dir, test_target_dir, tokenizer))"
   ],
   "id": "baebffd52ad34c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, device, train_loader, optimizer, scheduler, num_epochs, curr_epoch=0):\n",
    "    for epoch in range(curr_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap the train_loader with tqdm for a progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Load batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            labels_attention_mask = batch['labels_attention_mask'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids=input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=labels,\n",
    "                            decoder_attention_mask=labels_attention_mask)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update the learning rate\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update the progress bar with the current loss\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_epoch_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': avg_epoch_loss\n",
    "        }, os.path.join(checkpoint_dir, checkpoint_name+\".pt\"))\n",
    "        model.save_pretrained(checkpoint_name)\n",
    "            \n",
    "        # evaluate model performance every 5 epochs\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            bleu = eval_bleu(model, device, validation_source_dir, validation_target_dir, tokenizer)\n",
    "            print(\"validation BLEU: \", bleu)\n",
    "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_epoch_loss}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39822520cbb23be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Start training\n",
    "train(model, device, train_loader, optimizer, scheduler, num_epochs, curr_epoch=curr_epoch)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde269282f97a59b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# measure model performance after fine-tuning\n",
    "eval_bleu(model, device, test_source_dir, test_target_dir, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b69e42173100fce5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model\n",
    "model.save_pretrained(checkpoint_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1eff1055b378dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def distinct_with_beam_search(model,\n",
    "                              device,\n",
    "                              src_dir,\n",
    "                              tgt_dir,\n",
    "                              tokenizer,\n",
    "                              batch_size=16,\n",
    "                              beam_size=10,\n",
    "                              num_return_sequences=8):\n",
    "    model.eval()\n",
    "    source_codes = open(src_dir, encoding=\"utf-8\").readlines()\n",
    "    targets = open(tgt_dir, encoding=\"utf-8\").readlines()\n",
    "    source_codes = [code.rstrip() for code in source_codes]\n",
    "    targets = [target.rstrip() for target in targets]\n",
    "    all_summaries = []\n",
    "    for i in tqdm(range(0, len(source_codes), batch_size)):\n",
    "        batch = source_codes[i:i+batch_size]\n",
    "        input = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = input[\"input_ids\"].to(device)\n",
    "\n",
    "        generated_ids = model.generate(input_ids, max_length=100, num_beams=beam_size, num_return_sequences=num_return_sequences)\n",
    "        summaries = [tokenizer.decode(generated_ids[j], skip_special_tokens=True) for j in range(len(batch)*num_return_sequences)]\n",
    "        all_summaries.extend(summaries)\n",
    "    hypotheses = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", summary.strip().lower()[:-1]+' .')] for summary in all_summaries]))\n",
    "    # repeat targets for each generated sequence\n",
    "    repeated_targets = []\n",
    "    for target in targets:\n",
    "        repeated_targets.extend([target]*num_return_sequences)\n",
    "    references = dict(enumerate([[re.sub(r\"\\n{1,}|\\t{1,}|\\r{1,}\", \" \", target.strip().lower())] for target in repeated_targets]))\n",
    "    #calculate oracle scores\n",
    "    _, bleu, ind_bleu = corpus_bleu(hypotheses, references)\n",
    "    reshaped_bleu = np.array(list(ind_bleu.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_bleu = np.max(reshaped_bleu, axis=1)\n",
    "    print(\"Oracle bleu: \", np.mean(oracle_bleu) * 100)\n",
    "    rouge_calculator = Rouge()\n",
    "    rouge_l, ind_rouge = rouge_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_rouge = np.array(list(ind_rouge.values())).reshape(-1, num_return_sequences)\n",
    "    oracle_rouge = np.max(reshaped_rouge, axis=1)\n",
    "    print(\"Oracle rouge-l: \", np.mean(oracle_rouge) * 100)\n",
    "    meteor_calculator = Meteor()\n",
    "    meteor, ind_meteor = meteor_calculator.compute_score(references, hypotheses)\n",
    "    reshaped_meteor = np.array(list(ind_meteor)).reshape(-1, num_return_sequences)\n",
    "    oracle_meteor = np.max(reshaped_meteor, axis=1)\n",
    "    print(\"Oracle meteor: \", np.mean(oracle_meteor) * 100)\n",
    "\n",
    "    return hypotheses, references"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47c3e61c2fd3e6b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "num_return_sequences_list = [10, 20]\n",
    "for num_return_sequences in num_return_sequences_list:\n",
    "    hypotheses, references = distinct_with_beam_search(model,\n",
    "                                                       device,\n",
    "                                                       test_source_dir,\n",
    "                                                       test_target_dir,\n",
    "                                                       tokenizer,\n",
    "                                                       batch_size=4,\n",
    "                                                       beam_size=num_return_sequences,\n",
    "                                                       num_return_sequences=num_return_sequences)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed74cdbdc45bd27e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
